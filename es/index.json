[{"categories":null,"content":"Bio Father. Husbsand. Pastafari. The one who loves developing software. Manufactured in 1971 in Barcelona (Spain) Programming things for the last 30+ years. In love with C#, Java, SQL, software architecture, async patterns \u0026 more. Ex-teacher, Blogger, Speaker, Geek \u0026 family guy, living in Andorra since 2001. Microsoft MVP 2002-2016, MAP, MCC Active member and moderator in the good old MS newsgroups and forums for almost two decades. Founder of AndorraDotNet, the first dotNet user group in the country. Host of several events, like Geek-a-palooza. Now in a well deserved retirement ","date":"2020-05-01","objectID":"/es/about/:1:0","tags":null,"title":"About me","uri":"/es/about/"},{"categories":null,"content":"I like Coding. Family. Mountains. Sports. Food. Nature. Feminism. Human rights. Scifi. Fantasy. Traveling. Science. Atheism. ","date":"2020-05-01","objectID":"/es/about/:2:0","tags":null,"title":"About me","uri":"/es/about/"},{"categories":null,"content":"I dislike Anti-vaxxers. Racism. Religious fundamentalists. Pseudoscience. Lack of imagination or proud on ignorance. Politicians. ","date":"2020-05-01","objectID":"/es/about/:3:0","tags":null,"title":"About me","uri":"/es/about/"},{"categories":null,"content":"Tech profile Languages and technologies I‚Äôve learnt through the years. Language/Technology Start End Expertise Basic 1983 1988 ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Assembler 1986 1986 ‚≠ê C 1990 1997 ‚≠ê‚≠ê‚≠ê‚≠ê C++ 1992 2000 ‚≠ê‚≠ê‚≠ê Pascal 1992 1994 ‚≠ê‚≠ê‚≠ê ObjectPal 1994 1994 ‚≠ê Visual Basic 1994 2002 ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê SQL (TSQL/PS-SQL) 1994 - ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Clipper 1998 1999 ‚≠ê‚≠ê‚≠ê Delphi 1998 2002 ‚≠ê‚≠ê‚≠ê‚≠ê Java 1998 2004 ‚≠ê‚≠ê‚≠ê‚≠ê ActionScript 1999 1999 ‚≠ê Visual Basic.NET 2002 2010 ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê C# 2002 - ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Javascript 2008 - ‚≠ê‚≠ê‚≠ê F# 2012 2012 ‚≠ê‚≠ê Typescript 2018 2018 ‚≠ê‚≠ê CSS 2018 - ‚≠ê‚≠ê Python 2019 - ‚≠ê‚≠ê Go 2020 - ‚≠ê ","date":"2020-05-01","objectID":"/es/about/:4:0","tags":null,"title":"About me","uri":"/es/about/"},{"categories":null,"content":"Who am I? Speaking at dotnet conference View this post on Instagram A post shared by Lluis Franco (@francolluis) on Jan 1, 2019 at 1:12pm PST View this post on Instagram A post shared by Lluis Franco (@francolluis) on Jul 14, 2018 at 11:19am PDT View this post on Instagram A post shared by Lluis Franco (@francolluis) on Aug 19, 2018 at 1:30pm PDT View this post on Instagram A post shared by Lluis Franco (@francolluis) on Feb 12, 2018 at 6:36am PST View this post on Instagram A post shared by Lluis Franco (@francolluis) on Aug 9, 2019 at 12:43am PDT View this post on Instagram A post shared by Lluis Franco (@francolluis) on Aug 9, 2019 at 12:38am PDT View this post on Instagram A post shared by Lluis Franco (@francolluis) on Mar 2, 2019 at 5:43am PST ","date":"2020-05-01","objectID":"/es/about/:5:0","tags":null,"title":"About me","uri":"/es/about/"},{"categories":["Development","Parallel Series"],"content":"Ir al √≠ndice de la serie ","date":"2011-03-03","objectID":"/es/parallelseries03/:0:0","tags":["csharp","net framework","Task","Parallel","Async"],"title":"Parallel Series 03","uri":"/es/parallelseries03/"},{"categories":["Development","Parallel Series"],"content":"Elementos del sistema opertivo Cuando hablamos de programaci√≥n paralela conviene tener bastante claros algunos conceptos a nivel de sistema operativo. En este apartado trataremos de aclarar estos t√©rminos, ya que m√°s adelante los usaremos frecuentemente. ","date":"2011-03-03","objectID":"/es/parallelseries03/:1:0","tags":["csharp","net framework","Task","Parallel","Async"],"title":"Parallel Series 03","uri":"/es/parallelseries03/"},{"categories":["Development","Parallel Series"],"content":"Procesos (Processes) Toda aplicaci√≥n ejecut√°ndose en el sistema operativo existe dentro del contexto de un proceso, aunque no todos los procesos se corresponden con aplicaciones visibles. Basta abrir el administrador de tareas para comprobar que la lista de procesos es bastante mayor a la de aplicaciones. Eso es as√≠ porque pueden corresponderse a servicios, aplicaciones no visibles o porque algunas aplicaciones est√°n dise√±adas para crear varios procesos (hola +ponga aqu√≠ el nombre de su explorador favorito+ üòÑ). Procesos de Windows en el Administrador de Tareas Un proceso proporciona los recursos necesarios para ejecutar un programa. Contiene un espacio de memoria virtual, c√≥digo ejecutable, un contexto de seguridad, un identificador de proceso √∫nico, variables de entorno, y al menos un thread de ejecuci√≥n. Un proceso de Windows Cada proceso se inicia con un √∫nico hilo (thread en adelante), a menudo llamado thread principal. Pero puede crear threads adicionales, que pueden ser utilizados para encargarse de diferentes tareas. Hacer llamadas entre procesos resulta complejo y muy costoso en t√©rminos de rendimiento debido a que deben usarse mecanismos especiales como pipes, sockets o llamadas RPC (Remote procedure call). ","date":"2011-03-03","objectID":"/es/parallelseries03/:1:1","tags":["csharp","net framework","Task","Parallel","Async"],"title":"Parallel Series 03","uri":"/es/parallelseries03/"},{"categories":["Development","Parallel Series"],"content":"Dominios de aplicaci√≥n .NET (AppDomains) Al ser .NET una plataforma que ejecuta c√≥digo administrado, los procesos que se crean al ejecutar √©stas aplicaciones son un poco diferentes, ya que cuando se dise√±√≥ .NET una de las m√°ximas preocupaciones fue la de tratar de mejorar el manejo de los procesos cl√°sico o no administrados. Por ello se cre√≥ el concepto de dominio de aplicaci√≥n, que podr√≠a definirse como un proceso l√≥gico dentro del proceso del sistema operativo. Procesos en .NET Framework La gran diferencia es que dentro de un proceso podemos crear distintos dominios de aplicaci√≥n y cargar en cada uno de ellos varios ensamblados, y aprovechar que las llamadas entre distintos dominios de aplicaci√≥n y los ensamblados que contienen son mucho m√°s r√°pidas que entre procesos. Si uno de estos ensamblado debe ser compartido entre dos dominios de aplicaciones √©ste se copia en cada uno de los dominios. De este modo, al usar los dominios de aplicaci√≥n se obtiene la ventaja de aislar el c√≥digo de un proceso a otro, pero sin pagar el sobrecoste dedicado a realizar llamadas entre procesos. ","date":"2011-03-03","objectID":"/es/parallelseries03/:1:2","tags":["csharp","net framework","Task","Parallel","Async"],"title":"Parallel Series 03","uri":"/es/parallelseries03/"},{"categories":["Development","Parallel Series"],"content":"Hilos (Threads) Un thread es la entidad dentro de un proceso encargada de ejecutar el c√≥digo. Todos los threads que contiene un proceso comparten los recursos y memoria virtual de √©ste, y mantienen controladores de excepciones, una prioridad de programaci√≥n, almacenamiento local, y un identificador de thread √∫nico. Los threads son independientes a los dominios de aplicaci√≥n, de forma que podemos pensar en ellos como elementos transversales que pueden saltar de un uno a otro a lo largo del tiempo. No existe ninguna correspondencia entre el n√∫mero de threads y de dominios de aplicaci√≥n. Por defecto, todos los procesos se crean con un thread por defecto llamado thread principal, aunque en las aplicaciones .NET se crean al menos dos, ya que es necesario un segundo thread para administrar el recolector de basura. No obstante cada proceso puede crear un n√∫mero casi ilimitado de ellos, aunque en √∫ltima instancia el sistema operativo siempre tiene la potestad de priorizar a la baja estos hilos o incluso congelarlos. Threads dentro de un proceso Realizar cambios de contexto entre threads es much√≠simo m√°s r√°pido que los cambios de contexto de proceso. De hecho en los sistemas operativos que utilizan multitarea preemptiva (la gran mayor√≠a hoy en d√≠a) el sistema operativo va cediendo una peque√±a fracci√≥n de tiempo a cada uno de los threads de cada uno de los procesos cargados para que ejecuten una porci√≥n de su c√≥digo ejecutable, dando la sensaci√≥n de que varias aplicaciones se ejecutan al mismo tiempo. Este tipo de multitarea tiene la ventaja frente a sus predecesores de que si un proceso deja de responder, el sistema no se colapsa y puede seguir respondiendo sin verse afectado por la ca√≠da del mismo. Esto en la pr√°ctica ha significado la casi desaparici√≥n de las llamadas BSOD (Blue Screen Of Death) causadas por este motivo. ","date":"2011-03-03","objectID":"/es/parallelseries03/:1:3","tags":["csharp","net framework","Task","Parallel","Async"],"title":"Parallel Series 03","uri":"/es/parallelseries03/"},{"categories":["Development","Parallel Series"],"content":"Multihilo (Multithreading) A la capacidad que tienen los procesos de crear distintos threads ejecut√°ndose simult√°neamente es a lo que llamamos Multithreading. Y nos ha permitido simular la multitarea en los ordenadores personales de la √∫ltima d√©cada y media. Esto es as√≠ porque aunque f√≠sicamente s√≥lo haya un microprocesador, en t√©rminos del sistema operativo √©ste cede un periodo de tiempo a cada thread de cada uno de los procesos cargados en el sistema, y al repetirse una y otra muy r√°pidaamente vez produce la sensaci√≥n de que todas las aplicaciones se ejecutan al mismo tiempo, pero nada m√°s lejos de la realidad. Al menos hasta hace poco. Threads ejecut√°ndose en un core de procesdor ","date":"2011-03-03","objectID":"/es/parallelseries03/:1:4","tags":["csharp","net framework","Task","Parallel","Async"],"title":"Parallel Series 03","uri":"/es/parallelseries03/"},{"categories":["Development","Parallel Series"],"content":"Paralelismo (Parallelism) Con la aparici√≥n de los primeros microprocesadores con varios n√∫cleos (cores), al fin se pudo ejecutar c√≥digo en paralelo y obtener la tan deseada multitarea real, ya que distintos threads pueden ejecutarse en distintos cores al mismo tiempo. De modo que a m√°s cores, m√°s threads pueden ejecutarse y por consiguiente m√°s c√≥digo al mismo tiempo. Threads ejecut√°ndose en varios cores a la vez Hace apenas cuatro o cinco a√±os de la aparici√≥n de los primeros dual core, s√≥lo un poco m√°s tarde aparecieron los quad core, y hoy en d√≠a es bastante habitual ver estaciones de trabajo con 8 y hasta 16 cores. En cuanto al futuro, nadie sabe al ritmo que evolucionar√° esta tecnolog√≠a, pero los chicos de Intel hace m√°s de dos a√±os ya filtraron im√°genes de un Windows Server con 256 cores. Incluso los s√∫per ligeros procesadores para tel√©fonos y tablets basados en arquitectura ARM est√°n empezando a lanzar modelos de dos y cuatro cores. Qui√©n quiere uno de estos? Volviendo al ejemplo de los monos, resulta muy tentador pensar que si aprovechamos toda la potencia de los nuevos n√∫cleos podemos obtener ganancias de rendimiento espectaculares y escribir los 200 tomos en el tiempo que escribimos uno de ellos. Evidentemente esta afirmaci√≥n es un poco exagerada, ya que siempre va a haber un arduo trabajo de sincronizaci√≥n entre los diferentes monos‚Ä¶ perd√≥n threads. Con todo, la ganancia es espectacular, llegando f√°cilmente a multiplicar x5 o x6 en un entorno con 8 cores. Algo nada despreciable en seg√∫n qu√© procesos. De modo que viendo el n√∫mero de cores hacia d√≥nde nos movemos, a mi juicio se hace imperativo conocer -si no dominar- la TPL. ","date":"2011-03-03","objectID":"/es/parallelseries03/:1:5","tags":["csharp","net framework","Task","Parallel","Async"],"title":"Parallel Series 03","uri":"/es/parallelseries03/"},{"categories":["Development","Parallel Series"],"content":"Antes de terminar, un consejo Como hemos visto, por el momento s√≥lo puede existir paralelismo real en una estaci√≥n con varios cores. De otro modo el c√≥digo funcionar√° sin errores pero solo utilizando el √∫nico core de la m√°quina. As√≠ que un error bastante com√∫n entre los desarrolladores es utilizar m√°quinas virtuales para desarrollar, y olvidarnos que la mayor√≠a no permiten especificar varios cores. De modo que es bastante probable que alguna vez nos encontremos refunfu√±ando porque un c√≥digo bien escrito no obtiene ninguna ganancia cuando lo ejecutemos dentro de una maquina virtual üò† ","date":"2011-03-03","objectID":"/es/parallelseries03/:2:0","tags":["csharp","net framework","Task","Parallel","Async"],"title":"Parallel Series 03","uri":"/es/parallelseries03/"},{"categories":["Development","Parallel Series"],"content":"Video Aqu√≠ tienes un v√≠deo corto (15 minutos) sobre lo que acabamos de contar ;) A continuci√≥n‚Ä¶ En el pr√≥ximo post veremos c√≥mo extender LINQ con Paralel LINQ, y de este modo dotar a nuestras consultas sobre listas enumerables de paralelismo, sin apenas impacto en el c√≥digo actual. Ir al √≠ndice de la serie ","date":"2011-03-03","objectID":"/es/parallelseries03/:3:0","tags":["csharp","net framework","Task","Parallel","Async"],"title":"Parallel Series 03","uri":"/es/parallelseries03/"},{"categories":["Development","Parallel Series"],"content":"Ir al √≠ndice de la serie ","date":"2011-01-21","objectID":"/es/parallelseries02/:0:0","tags":["csharp","net framework","Task","Parallel","Async"],"title":"Parallel Series 02","uri":"/es/parallelseries02/"},{"categories":["Development","Parallel Series"],"content":"Un poco de historia La programaci√≥n paralela no es nada nuevo. Ya estaba presente all√° en mis tiempos de estudiante hace m√°s de 20 a√±os üò¢ y hoy en d√≠a, desde la aparici√≥n del .Net Framework 4.0 est√° m√°s viva que nunca gracias a la Task Parallel Library o TPL. No obstante, decir que la TPL s√≥lo sirve para realizar tareas as√≠ncronas es como decir que un smartphone s√≥lo sirve para llamar por tel√©fono. Es m√°s, much√≠simo m√°s. Y es precisamente, de la mano de √©sta librer√≠a que vamos a introducirnos en el apasionante mundo de la programaci√≥n paralela. Esta disciplina siempre ha estado tradicionalmente asociada a los perfiles t√©cnicos m√°s elevados y reservada para ocasiones especiales. Sin embargo a partir de ahora y gracias a la TPL va a ser accesible a todo tipo de desarrolladores, y se va a convertir en algo muy importante, algo que todo buen desarrollador deber√° a√±adir a su lista de activos. De hecho, va a ser una parte esencial en el futuro inmediato del desarrollo de aplicaciones a todos los niveles. Pero ¬øqu√© realmente es la programaci√≥n paralela? Podemos pensar en ella como en la posibilidad de dividir una tarea larga y pesada en varias tareas m√°s cortas, y ejecutar √©stas al mismo tiempo, de modo que tarde mucho menos que la tarea original. ","date":"2011-01-21","objectID":"/es/parallelseries02/:1:0","tags":["csharp","net framework","Task","Parallel","Async"],"title":"Parallel Series 02","uri":"/es/parallelseries02/"},{"categories":["Development","Parallel Series"],"content":"Enciclopedias y monos Supongamos que tenemos que copiar los 200 tomos de la gran enciclopedia gal√°ctica de T√©rminus. L√≥gicamente no es lo mismo copiarlos uno tras otro, que contratar a 200 monos mutantes entrenados para copiar, y sentarlos en 200 escritorios con sus 200 bol√≠grafos a copiar los 200 libros. Es evidente que -de ser posible- la segunda opci√≥n ser√≠a mucho m√°s r√°pida. A m√°s recursos (escritorios y bol√≠grafos) m√°s r√°pido terminaremos la tarea Pero ¬øqu√© sucede si s√≥lo tenemos 100 escritorios y bol√≠grafos? Pues que los monos van a tener que hacer cola y esperar su turno, de modo que cuando uno de los monos termine o se canse de escribir, deber√° ceder su turno al mono que espera, provocando por el camino algunas colas y enfados por parte de los monos, que son buenos trabajadores pero un poco particulares. Con todo, a menos que estalle una guerra siempre ser√° m√°s r√°pido que la primera opci√≥n, pero eso nos deja ya la primera conclusi√≥n: a m√°s recursos (escritorios y bol√≠grafos) m√°s r√°pido terminaremos la tarea. Y de paso vamos a tener que preocuparnos menos por gestionar los turnos y las esperas de los monos, con todo lo que conlleva. Porque como veremos m√°s adelante, en muchas ocasiones cuando trabajamos con monos o con threads el tiempo de sincronizaci√≥n es primordial, y puede marcar la diferencia entre el √©xito y el fracaso de nuestra aplicaci√≥n. Monos programando ","date":"2011-01-21","objectID":"/es/parallelseries02/:2:0","tags":["csharp","net framework","Task","Parallel","Async"],"title":"Parallel Series 02","uri":"/es/parallelseries02/"},{"categories":["Development","Parallel Series"],"content":"Leyes muertas y f√≠sica cu√°ntica Sin embargo, hasta ahora nos ha ido muy bien con la programaci√≥n tradicional as√≠ que ¬øporque parece ser que ahora es cuesti√≥n de vida o muerte aprender este nuevo paradigma? Es decir, habitualmente hasta ahora no hab√≠an demasiadas ocasiones en las que una aplicaci√≥n deb√≠a recurrir a la asincron√≠a o al paralelismo (que como veremos m√°s adelante no es exactamente lo mismo). Por qu√© ahora s√≠? La materia presenta efectos cu√°nticos que har√≠an necesaria una tecnolog√≠a diferente para seguir realizando c√°lculos a ese nivel La respuesta es sencilla, parece que la buena vida se termina. Si ha habido una constante en IT en los √∫ltimos 45 a√±os, esa es la que describe la ley de Moore: En 1965 Gordon Moore, uno de los fundadores de Intel predijo que cada 2 a√±os (18 meses al principio) se doblar√≠a el n√∫mero de componentes de un circuito integrado. Y se ha cumplido a rajatabla hasta hoy, aunque en los √∫ltimos a√±os se est√°n alcanzando ciertos l√≠mites que hacen que dicha ley no pueda seguir cumpli√©ndose. Ley de moore Simplificando un poco podr√≠amos decir que existen un par de problemas: El escalado de frecuencia de los microprocesadores y el calor generado por los mismos. El primero de ellos hace referencia a la dificultad de seguir incrementando la velocidad de los microprocesadores, debido principalmente a que la tecnolog√≠a utilizada para dise√±arlos est√° actualmente cerca de los 32 nan√≥metros y el l√≠mite f√≠sico antes de que la materia experimente cambios, se calcula que est√° entre los 22 y los 18 nan√≥metros. Esta previsto alcanzar este l√≠mite aproximadamente en s√≥lo dos o tres a√±os, hacia 2014. A la vuelta de la esquina. Una vez alcanzado ese nivel de miniaturizaci√≥n, en palabras del cient√≠fico Stephen Hawking: ‚ÄúLa materia presenta efectos cu√°nticos que har√≠an necesaria una tecnolog√≠a diferente para seguir realizando c√°lculos a ese nivel‚Äù. El segundo de los problemas va ligado al primero, y es que en los √∫ltimos a√±os el calor generado por los microprocesadores se ha ido incrementado exponencialmente, y en t√©rminos de densidad de potencia ya es igual al calor generado por la tobera de un cohete. Lo peor de todo ello es que incrementar la frecuencia s√≥lo entre un 5 y 10 por ciento cada a√±o, tiene un coste de casi doblar la temperatura. El futuro cercano Con esto no quiero decir que no puedan fabricarse ordenadores m√°s r√°pidos en un futuro. Quiero decir que si estas predicciones son acertadas, no podr√°n fabricarse microprocesadores m√°s r√°pidos con la tecnolog√≠a actual. Tal vez sea posible si se descubre c√≥mo construir ordenadores que utilicen tecnolog√≠a √≥ptica, nano-ingenier√≠a para crear transistores basados en nanotubos que aprovechen el llamado efecto t√∫nel, o cualquier otro concepto a√∫n por descubrir. Pero por el momento no podemos contar con ello. ","date":"2011-01-21","objectID":"/es/parallelseries02/:3:0","tags":["csharp","net framework","Task","Parallel","Async"],"title":"Parallel Series 02","uri":"/es/parallelseries02/"},{"categories":["Development","Parallel Series"],"content":"Deus ex machina Si por algo se ha caracterizado el ser humano es por su gran habilidad en resolver problemas (dejando aparte su nada desde√±able habilidad para provocarlos), de modo que ya hace unos a√±os que se ha empezado a desarrollar y fabricar una de las soluciones a este problema. De hecho hoy en d√≠a se ha convertido en algo casi cotidiano: Se trata de fabricar procesadores con varios n√∫cleos, que se repartan el trabajo -como los monos- consiguiendo as√≠ aumentar la velocidad. No por el hecho de ser cada vez m√°s r√°pidos, si no por existir cada vez m√°s recursos trabajando al mismo tiempo. Algo parecido -salvando las distancias- al cerebro humano, que en comparaci√≥n con un ordenador es bastante m√°s lento, pero su capacidad de procesamiento en paralelo gracias a sus millones de conexiones entre neuronas, no tiene rival con ning√∫n otro elemento conocido en la naturaleza ni creado por el hombre. Que nos depara el futuro? A continuci√≥n‚Ä¶ En el pr√≥ximo post aclararemos algunos conceptos b√°sicos aunque necesarios cuando desarrollamos aplicaciones que hagan uso de la programaci√≥n as√≠ncrona. Ir al √≠ndice de la serie ","date":"2011-01-21","objectID":"/es/parallelseries02/:4:0","tags":["csharp","net framework","Task","Parallel","Async"],"title":"Parallel Series 02","uri":"/es/parallelseries02/"},{"categories":["Development","Parallel Series"],"content":"Ir al √≠ndice de la serie ","date":"2011-01-11","objectID":"/es/parallelseries01/:0:0","tags":["csharp","net framework","Task","Parallel","Async"],"title":"Parallel Series 01","uri":"/es/parallelseries01/"},{"categories":["Development","Parallel Series"],"content":"Prologo Cada versi√≥n del .NET framework nos sorprende con una serie de novedades, y para cada uno de nosotros hay -al menos- una que nos roba el coraz√≥n. A mi me sucedi√≥ con la aparici√≥n de Generics con el framework 2.0, los m√©todos extensores y LINQ en las versiones 3.0 y 3.5 y me ha pasado de nuevo con la Task Parallel Library en la versi√≥n 4.0. Pens√°ndolo detenidamente tampoco no es tan extra√±o, al fin y al cabo siempre me ha gustado la programaci√≥n as√≠ncrona (algo que si alguien le llega a contar a alguno de mis primeros maestros de tecnolog√≠a digital, se habr√≠a muerto de la risa). Sin embargo, con los a√±os √©ste que escribe ha llegado -m√°s por tozudez que por talento natural- a conocer un poco los entresijos de la programaci√≥n as√≠ncrona. Una disciplina en la que tan importante es saber lo que hay que hacer, como lo que no hay que hacer. As√≠ pues, cuando lleg√≥ a mis manos la primera preview de Visual Studio 2010, una de las primeras cosas que hice fue mirar que demonios era eso de la Task Parallel Library. Primero porque ya hac√≠a un tiempo que hab√≠a escuchado acerca de la muerte anunciada de la ley de Moore. Y segundo porque cualquier cosa que hiciese m√°s llevadero el trabajo de realizar y depurar aplicaciones multihilo, bienvenido iba a ser. Task Parallel Library Desde entonces hasta ahora he tenido la suerte de poder dedicar un poco de tiempo a esta maravilla, de modo que me he propuesto crear una serie bastante larga de posts y v√≠deos sobre el tema. M√°s que nada porque a mi juicio hay poca documentaci√≥n (al menos en Espa√±ol) y algo as√≠ no puede quedar relegado al olvido. Y es que en multitud de ocasiones que cuando le explico a alguien el porqu√© es importante y en que consiste la TPL, acostumbra a decir: ‚ÄúQue guapo! Pues no ten√≠a ni idea‚Ä¶‚Äù. Y eso, me mata. ","date":"2011-01-11","objectID":"/es/parallelseries01/:1:0","tags":["csharp","net framework","Task","Parallel","Async"],"title":"Parallel Series 01","uri":"/es/parallelseries01/"},{"categories":["Development","Parallel Series"],"content":"Nacen las Parallel Series En los pr√≥ximos posts voy a ir definiendo el √≠ndice de contenidos de la serie, aunque a medida que crezca la serie es m√°s que probable que vaya siendo modificado y ampliado. Tambi√©n veremos algo de historia para ponernos en contexto, aclararemos conceptos base que van a ser necesarios m√°s adelante, e iremos desgranando los apartados principales de la Task Parallel Library. La idea es empezar desde abajo e ir subiendo de nivel, hasta llegar a los aspectos m√°s complejos, como los problemas de concurrencia o la depuraci√≥n de este tipo de aplicaciones. En cada apartado prometo poner al menos un ejemplo y si puedo m√°s, porque los humanos -y los developers heredamos de esta clase base- aprendemos mucho mejor los conceptos te√≥ricos si van acompa√±ados de la pr√°ctica. Nos vemos muy pronto ;-) A continuci√≥n‚Ä¶ En el pr√≥ximo post repasaremos la historia de la programaci√≥n paralela y veremos c√≥mo hemos llegado aqu√≠. Ir al √≠ndice de la serie ","date":"2011-01-11","objectID":"/es/parallelseries01/:2:0","tags":["csharp","net framework","Task","Parallel","Async"],"title":"Parallel Series 01","uri":"/es/parallelseries01/"},{"categories":["Development","Parallel Series"],"content":"√çndice de contenidos 01 - El Alfa (Pr√≥logo) 02 - Un poco de historia 03 - Aclarando conceptos base 04 - PLINQ: Parallel LINQ 05 - Parallel static class 06 - Tasks, la 8¬™ maravilla 07 - Problemas de concurrencia 08 - Materiales y presentaciones de mis eventos ","date":"2011-01-10","objectID":"/es/parallelseriesindex/:1:0","tags":["csharp","net framework","Task","Parallel","Async"],"title":"Parallel Series - Index","uri":"/es/parallelseriesindex/"},{"categories":["Development","Parallel Series"],"content":"Relacionados Programaci√≥n funcional para el resto de nosotros Luces, c√°mara‚Ä¶ Action! ","date":"2011-01-10","objectID":"/es/parallelseriesindex/:2:0","tags":["csharp","net framework","Task","Parallel","Async"],"title":"Parallel Series - Index","uri":"/es/parallelseriesindex/"}]