[{"categories":null,"content":"Bio Father. Husbsand. Pastafari. The one who loves developing software. Manufactured in 1971 in Barcelona (Spain) Programming things for the last 30+ years. In love with C#, Java, SQL, software architecture, async patterns \u0026 more. Ex-teacher, Blogger, Speaker, Geek \u0026 family guy, living in Andorra since 2001. Microsoft MVP 2002-2016, MAP, MCC Active member and moderator in the good old MS newsgroups and forums for almost two decades. Founder of AndorraDotNet, the first dotNet user group in the country. Host of several events, like Geek-a-palooza. Now in a well deserved retirement ","date":"2020-05-01","objectID":"/es/about/:1:0","tags":null,"title":"About me","uri":"/es/about/"},{"categories":null,"content":"I like Coding. Family. Mountains. Sports. Food. Nature. Feminism. Human rights. Scifi. Fantasy. Traveling. Science. Atheism. ","date":"2020-05-01","objectID":"/es/about/:2:0","tags":null,"title":"About me","uri":"/es/about/"},{"categories":null,"content":"I dislike Anti-vaxxers. Racism. Religious fundamentalists. Pseudoscience. Lack of imagination or proud on ignorance. Politicians. ","date":"2020-05-01","objectID":"/es/about/:3:0","tags":null,"title":"About me","uri":"/es/about/"},{"categories":null,"content":"Tech profile Languages and technologies I’ve learnt through the years. Language/Technology Start End Expertise Basic 1983 1988 ⭐⭐⭐⭐⭐ Assembler 1986 1986 ⭐ C 1990 1997 ⭐⭐⭐⭐ C++ 1992 2000 ⭐⭐⭐ Pascal 1992 1994 ⭐⭐⭐ ObjectPal 1994 1994 ⭐ Visual Basic 1994 2002 ⭐⭐⭐⭐⭐ SQL (TSQL/PS-SQL) 1994 - ⭐⭐⭐⭐⭐ Clipper 1998 1999 ⭐⭐⭐ Delphi 1998 2002 ⭐⭐⭐⭐ Java 1998 2004 ⭐⭐⭐⭐ ActionScript 1999 1999 ⭐ Visual Basic.NET 2002 2010 ⭐⭐⭐⭐⭐ C# 2002 - ⭐⭐⭐⭐⭐ Javascript 2008 - ⭐⭐⭐ F# 2012 2012 ⭐⭐ Typescript 2018 2018 ⭐⭐ CSS 2018 - ⭐⭐ Python 2019 - ⭐⭐ Go 2020 - ⭐ ","date":"2020-05-01","objectID":"/es/about/:4:0","tags":null,"title":"About me","uri":"/es/about/"},{"categories":null,"content":"Who am I? Speaking at dotnet conference View this post on Instagram A post shared by Lluis Franco (@francolluis) on Jan 1, 2019 at 1:12pm PST View this post on Instagram A post shared by Lluis Franco (@francolluis) on Jul 14, 2018 at 11:19am PDT View this post on Instagram A post shared by Lluis Franco (@francolluis) on Aug 19, 2018 at 1:30pm PDT View this post on Instagram A post shared by Lluis Franco (@francolluis) on Feb 12, 2018 at 6:36am PST View this post on Instagram A post shared by Lluis Franco (@francolluis) on Aug 9, 2019 at 12:43am PDT View this post on Instagram A post shared by Lluis Franco (@francolluis) on Aug 9, 2019 at 12:38am PDT View this post on Instagram A post shared by Lluis Franco (@francolluis) on Mar 2, 2019 at 5:43am PST ","date":"2020-05-01","objectID":"/es/about/:5:0","tags":null,"title":"About me","uri":"/es/about/"},{"categories":["Development","Fundamentals"],"content":"Ir al índice de la serie ","date":"2012-02-10","objectID":"/es/functional-programming-for-the-rest-of-us/:0:0","tags":["Functional","Programming","Lambda","History"],"title":"Programación funcional para el resto de nosotros","uri":"/es/functional-programming-for-the-rest-of-us/"},{"categories":["Development","Fundamentals"],"content":"Mi idea inicial Originalmente, en mi serie sobre la Task Parallel Library estaba pensando en incluir algún post sobre programación funcional, pero cuando encontré esta pequeña joya decidí pedir permiso a Luis para poder incluir una copia en mi blog, ya que ni en sueños podría yo haber escrito algo tan completo. Mis felicitaciones! Nota Artículo reproducido con permiso del traductor original. Dale un vistazo al excelente trabajo y consulta la versión traducida de Luis Mendoza, basado en el post original de Slava Akhmechet. Sin más os dejo con uno de los mejores artículos que he podido leer últimamente. A disfrutar! ","date":"2012-02-10","objectID":"/es/functional-programming-for-the-rest-of-us/:1:0","tags":["Functional","Programming","Lambda","History"],"title":"Programación funcional para el resto de nosotros","uri":"/es/functional-programming-for-the-rest-of-us/"},{"categories":["Development","Fundamentals"],"content":"Artículo original ","date":"2012-02-10","objectID":"/es/functional-programming-for-the-rest-of-us/:2:0","tags":["Functional","Programming","Lambda","History"],"title":"Programación funcional para el resto de nosotros","uri":"/es/functional-programming-for-the-rest-of-us/"},{"categories":["Development","Fundamentals"],"content":"Introducción Los programadores son procrastinadores (o sea, personas que aplazan las cosas). Llegan, toman un poco de café, revisan su bandeja de entrada, leen sus actualizaciones de RSS, leen las noticias, dan un vistazo a los artículos más recientes en los sitios de tecnología, examinan las discusiones políticas en los secciones designadas de los foros de programación… se restriegan los ojos y echan otro vistazo para asegurarse de no perderse nada. Van a comer. Regresan, inician el IDE por unos minutos. Revisan la bandeja de entrada. Toman un poco de café. Antes de darse cuenta, el día de trabajo ya terminó. Pero, de vez en cuando, te encuentras con artículos verdaderamente desafiantes. Si buscas en el lugar correcto encontrarás al menos uno cada pocos días. Como son difíciles de entender y necesitas tiempo para leerlos, empiezan a acumularse. Antes de darte cuenta, tienes una larga lista de vínculos y una carpeta llena de archivos PDF y quisieras tener un año en una pequeña cabaña a la mitad del bosque sin nadie a kilómetros a la redonda para que finalmente puedas comprenderlos. Estaría bien que alguien viniera cada mañana mientras das un paseo por el río para dejarte algo de comida y llevarse la basura. No sé de tu lista, pero una buena parte de la mía tiene que ver con programación funcional. Estos son generalmente los artículos más difíciles de entender. Escritos en un áspero lenguaje académico, aun los “veteranos de la industria de Wall Street con diez años de experiencia” no entienden de qué tratan los artículos sobre programación funcional (también llamada FP, por sus siglas en inglés). Si preguntas al administrador de proyectos de Citi Group o de Deutsche Bank por qué usan JMS en lugar de Erlang te dirán que no pueden usar lenguajes académicos para aplicaciones de fortaleza industrial (Cuando buscaba trabajo en el otoño de 2005 a menudo hice esa pregunta. Es bastante divertido recordar cuantos rostros con signos de interrogación vi. Uno podría pensar que a $300,000 la pieza de estas personas, al menos tendrían un buen entendimiento de las herramientas que tienen disponibles). El problema es que algunos de los sistemas más complejos con los requerimientos más rígidos están escritos usando elementos de programación funcional. Algo no cuadra. Es cierto que los artículos sobre FP (recuerda que son las siglas en inglés para programación funcional) son difíciles de entender, pero no tendrían por qué serlo. Las razones para que haya ese obstáculo al conocimiento son meramente históricas. No hay nada inherentemente difícil en los conceptos de FP. Considera este artículo como “una guía accesible para entener FP”, un puente entre nuestras mentes imperativas hacia el mundo de FP. Prepárate un café y sigue leyendo. Con un poco de suerte, en poco tus amigos estarán bromeando sobre tus nuevas ideas sobre FP. Así que, ¿qué es la programación funcional o FP? ¿Cómo se produce? ¿Es comestible? Si es tan útil como proclaman sus defensores, ¿por qué no es más usada en la industria? ¿Por qué parece que solo personas con grado de doctorado lo quieren usar? Más importante, ¿por qué es tan difícil de aprender? ¿Qué es todo eso de cierres (closures), continuaciones, currying, evaluación tardía y cero efectos colaterales? ¿Cómo puede usarse en proyectos que no tengan que ver con universidades? ¿Por qué parece tan distinto de todo lo bueno, santo y querido por nuestros corazones imperativos? Aclararemos todo esto pronto. Empecemos explicando las razones por las que existe esa gran barrera entre el mundo real y los artículos académicos. La respuesta es tan sencilla como dar un paseo por el parque. ","date":"2012-02-10","objectID":"/es/functional-programming-for-the-rest-of-us/:2:1","tags":["Functional","Programming","Lambda","History"],"title":"Programación funcional para el resto de nosotros","uri":"/es/functional-programming-for-the-rest-of-us/"},{"categories":["Development","Fundamentals"],"content":"Un paseo por el parque ¡Enciende la máquina del tiempo! Nuestro paseo empieza unos dos mil años atrás, en una bello y soleado día de la primavera de 380 a.C. A las afueras de las murallas de Atenas, bajo la plácida sombra de los olivos, Platón caminaba rumbo a la Academia con un joven pupilo. El clima estaba agradable, la cena había estado deliciosa, y la conversación empezó a tomar tintes filosóficos. “Mira a esos dos estudiantes”, dijo Platón escogiendo cuidadosamente las palabras para hacer una pregunta educativa. “¿Cual de ellos te parece más alto?” El joven pupilo miró hacia la cuenca de agua en la que dos hombres estaban parados. “Son más o menos de la misma altura”, contestó. Platón preguntó: “¿Qué quieres decir con ‘más o menos’?”. El joven contesto: “Bueno, desde aquí se ven de la misma estatura, pero si estuviera más cerca seguramente vería alguna diferencia”. Platón sonrió. Había llevado al joven en la dirección correcta. “¿Así que dirías que no hay ninguna cosa perfectamente igual a otra en nuestro mundo?” Después de pensar un poco, el joven respondió: “No lo creo. Toda cosa es al menos un poco diferente de otra, aunque no podamos ver la diferencia”. ¡Había dado en el clavo! Platón dijo: “Entonces, si ninguna cosa es perfectamente igual a otra en este mundo, ¿cómo crees que entendemos el concepto de equidad ‘perfecta’?” El joven se quedó perplejo. “No lo sé”, contestó. Así nacio el primer esfuerzo por entender la naturaleza de las matemáticas. Platón sugirió que todo en nuestro mundo es solo una aproximación de la perfección. Además, se dio cuenta de que entendemos el concepto de perfección aunque no la hayamos visto. Llegó a la conclusión de que las formas matemáticas perfectas viven en otro mundo y que de alguna forma sabemos de ellas al tener una conexión con ese universo “alternativo”. Sabemos bien que no hay un círculo perfecto que podamos observar. Pero entendemos qué es un círculo perfecto y podemos describirlo mediante ecuaciones. ¿Qué son, entonces, las matemáticas? ¿Por qué esta descrito nuestro universo con leyes matemáticas? ¿Será posible que todos los fenómenos del universo sean descritos mediante las matemáticas? (Esta parece ser una cuestión muy controversial. Los físicos y los matemáticos se ven obligados a reconocer que no esta del todo claro si todo en el universo obedece leyes matemáticas o no.) La filosofía de las matemáticas es una materia de estudio muy compleja. Como muchas disciplinas filosóficas genera más preguntas que respuestas. Buena parte de los concensos alcanzados giran en torno a que las matemáticas son realmente un rompecabezas: podemos declaramos un conjunto básico de principios que no entran en conflicto, y un conjunto de reglas sobre cómo operan estos principios. Entonces podemos usar estas reglas como base para reglas más y más complejas. Los matemáticos le llaman a esto un “sistema formal” o “cálculo”. Podríamos construir un sistema formal del Tetris si quisieramos. De hecho, una implementación del Tetris que funcione es un sistema formal, solo que especificado usando una representación inusual. Una civilización de criaturas peludas que existiera en Alfa Centauri no podría leer nuestros formalismos del Tetris y de los círculos porque su único órgano sensorial solo percibiera olores. Lo más probable es que nunca sabrían nada del Tetris, pero sí tendrían un formalismo para los círculos. Probablemente nosotros no podríamos entenderlo, pues nuestro sentido del olfato no sería tan sofisticado, pero una vez que dejamos de lado la representación del formalismo (a través de diversos instrumentos sensoriales y técnicas de lectura para entender el lenguaje), los conceptos fundamentales son entendibles para cualquier civilización inteligente. Es interesante notar que aunque no existiera una civilización inteligente en el universo, los formalismos del Tetris y de los círculos estarían ahí para ser examinados, solo que nadie los estaría buscando. Si una civilización inteligente apareciese,","date":"2012-02-10","objectID":"/es/functional-programming-for-the-rest-of-us/:2:2","tags":["Functional","Programming","Lambda","History"],"title":"Programación funcional para el resto de nosotros","uri":"/es/functional-programming-for-the-rest-of-us/"},{"categories":["Development","Fundamentals"],"content":"Un poco de historia (Siempre he odiado las lecciones de historia que presentan cronológicamente fechas, nombres y eventos de forma mecánica. Para mí, la historia es sobre las vidas de las personas que cambiaron el mundo. Es sobre sus razones personales que les llevaron a actuar de cierta forma, y los mecanismos por medio de los cuales cambiaron millones de vidas. Por esta razón, esta sección histórica es irremediablemente incompleta. Solo las personas y los eventos importantes son discutidos.) Cambiemos de velocidad en la máquina del tiempo. Esta vez viajaremos a un punto más cercano, a la decada de 1930. La Gran Depresión estaba asolando a todo el mundo. Casi toda familia de toda clase social se vió afectada por la tremenda recesión económica. Muy pocos santuarios quedaron donde la gente estaba a salvo de la pobreza. Pocas personas tuvieron la fortuna de estar en alguno de esos santuarios, pero existían. Nuestro interés se centrará en los matemáticos de la Universidad de Princeton. Las nuevas oficinas construidas en estilo gótico daban a Princeton un aire de paraíso. Lógicos (de la rama matemática de la lógica) de todo el mundo fueron invitados a Princeton para construir un nuevo departamento. Mientras muchos en norteamérica no podían ni poner una pieza de pan en sus mesas para la cena, techos altos, paredes cubiertas de madera tallada, discusiones diarias con una taza de té, y paseos por el bosque eran algunas de las condiciones de Princeton. Uno de los matemáticos viviendo ese lujoso estilo de vida fue un joven llamado Alonzo Church. Alonzo recibió un grado académico en Princeton, y fue persuadido a quedarse para un postgrado. Alonzo sentía que la arquitectura del lugar era más un lujo que algo necesario. Rara vez se le veía discutiendo sobre matemáticas con una taza de té, y no le gustaban los paseos por el bosque. Alonzo era solitario. Era más productivo cuando trabajaba por su cuenta. No obstante, Alonzo tenía contacto regular con otros habitantes de Princeton, entre ellos Alan Turing, John von Neumann, y Kurt Gödel. Los cuatro estaban interesados en los sistemas formales. No prestaban mucha atención al mundo físico; les resultaban más interesantes problemas con rompecabezas matemáticos abstractos. Sus rompecabezas tenían algo en común: los cuatro estaban interesados en responder preguntas sobre computación. Si tuvieramos máquinas con infinito poder computacional, ¿Que problemas podríamos resolver? ¿Podrían darse soluciones automáticamente? ¿Quedarían algunos problemas sin resolver? ¿Por qué? ¿Podrían maquinas con diferentes diseños ser iguales en poder? En cooperación con otros, Alonzo desarrolló un sistema formal llamado cálculo lambda. El sistema era esencialmente un lenguaje de programación para una de esas máquinas imaginarias. Se basaba en funciones que tomaban otras funciones como parámetros, y que devolvían funciones como resultado. La función en su papel de materia prima fue identificada con la letra griega lambda(λ) ahí el nombre del sistema (Cuando estaba aprendiendo programación funcional me quedé muy confundido por el término “lamda” porque no sabía qué significaba realmente. En este contexto lambda es una función. El uso de la letra griega es porque era más fácil escribirla en notación matemática. Cada vez que escuches el término “lambda” cuando se habla de programación funcional traducelo en tu mente como “función”). Usando este formalismo, Alonzo pudo razonar sobre muchas de las cuestiones antes planteadas, y llegó a respuestas concluyentes. Independientemente de Alonzo, Alan Turing realizó un trabajo similar. Desarrolló un formalismo diferente (ahora conocido como la máquina de Turing), y lo usó para llegar a conclusiones similares a las de Alonzo. Después se demostró que la máquina de Turing y el cálculo lamda son equivalentes en poder. Aquí es donde la historia terminaría. Yo terminaría el artículo, y tu navegarías a otra página, si no fuera por el comienzo de la Segunda Guerra Mundial. El mundo estaba","date":"2012-02-10","objectID":"/es/functional-programming-for-the-rest-of-us/:2:3","tags":["Functional","Programming","Lambda","History"],"title":"Programación funcional para el resto de nosotros","uri":"/es/functional-programming-for-the-rest-of-us/"},{"categories":["Development","Fundamentals"],"content":"Programación Funcional La programación funcional es la implementación práctica de las ideas de Alonzo Church. No todas las ideas del cálculo lambda se implementan en la práctica porque el cálculo lambda no fue diseñado para trabajar bajo limitaciones físicas. Por tanto, como en la programación orientada a objetos, la programación funcional es un conjunto de ideas, no un conjunto estricto de reglas. Hay muchos lenguajes de programación funcional, y la mayoría hacen las cosas de formas muy diferentes entre sí. En este artículo explicaré las ideas más ampliamente usadas en los lenguajes funcionales usando ejemplos tomados del lenguaje Java (sí, puedes escribir programas funcionales en Java si eres masoquista). En las siguientes secciones tomaremos Java tal cual, y le haremos modificaciones para transformarlo en lenguaje funcional útil. Empecemos nuestro viaje. El cálculo lambda fue creado para investigar problemas relacionados con cálculo. La programación funcional, por tanto, trata principalmente con cálculo, y, ¡sorpresa!, lo hace mediante funciones. La función es la unidad básica en programación funcional. Las funciones son usadas para prácticamente todo, aun para los cálculos más simples. Hasta las variables son reemplazadas con funciones. En programación funcional las variables son simplemente accesos directos a expresiones (para no tener que escribir todo en una misma línea). No pueden ser modificadas. Todas las variables pueden ser asignadas solo una vez. En términos de Java esto significa que cada variable es declarada como final (o const si hablamos de C++). No hay variables mutables en FP: final int i = 5; final int j = i + 3; Dado que cada variable en FP es final, podemos llegar a dos conclusiones interesantes. Primero, que no tiene sentido escribir la palabra clave final y segundo, que no tiene sentido llamar a las variables, pues… variables. Haremos estas dos modificaciones a Java: cada variable declarada en nuestro Java funcional sera final por default, y nos referiremos a las variables como símbolos. Para ahora probablemente te estas preguntando cómo podrías escribir algo razonablemente complicado en nuestro lenguaje recién creado. ¡Si todo símbolo es inmutable, entonces no podemos cambiar el estado de nada! Esto no es estrictamente cierto. Cuando Alonzo estaba trabajando en el cálculo lambda no estaba interesado en mantener un estado para ser modificado posteriormente. Estaba interesado en realizar operaciones sobre datos (tambien conocidos como “material de cálculo”). Como sea, el cálculo lambda es equivalente en poder a la máquina de Turing. Un lenguaje funcional puede hacer lo mismo que un lenguaje imperativo. ¿Cómo, entonces, podemos obtener los mismos resultados? En realidad los programas funcionales pueden mantener un estado, pero no usan las variables para eso. Usan funciones. El estado es mantenido en los parámetros de la función, en la pila. Si deseas mantener un estado para modificarlo posteriormente, escribes una función recursiva. Como ejemplo, escribamos una función que devuelva una cadena de carácteres de Java al revés. Recuerda que cada variable (más bien, cada símbolo) es final por default (Es interesante notar que las cadenas de Java son inmutables de todas formas. Es aun más interesante explorar las razones de esta falsedad, pero nos distraeríamos de nuestro objetivo). String al_reves(String arg) { if (arg.length() == 0) { return arg; } else { return al_reves( arg.substring(1, arg.length()) + arg.substring(0, 1)); } } Esta función es lenta porque se llama a sí misma repetidamente (Muchos de los compiladores de lenguajes funcionales optimizan las funciones recursivas transformandolas en sus contrapartes iterativas siempre que sea posible. A eso se le llama optimización de llamadas a la inversa o Tail recursion). Es una devoradora de memoria porque repetidamente asigna memoria a los objetos. Pero está escrita en estilo funcional. Quizá te preguntes porque alguien querría un programa de esta forma. ","date":"2012-02-10","objectID":"/es/functional-programming-for-the-rest-of-us/:2:4","tags":["Functional","Programming","Lambda","History"],"title":"Programación funcional para el resto de nosotros","uri":"/es/functional-programming-for-the-rest-of-us/"},{"categories":["Development","Fundamentals"],"content":"Que sigue? Este artículo solo toca la superficie de la programación funcional. A veces un pequeño esbozo puede convertirse en algo grande, y en nuestro caso eso es algo muy bueno. En el futuro planeo escribir sobre teoría de categorías, monads, estructuras de datos funcionales, sistemas de escritura en lenguajes funcionales, concurrencia funcional, bases de datos funcionales y mucho más. Si logro escribir (y en el proceso, aprender) sobre la mitad de estos temas, mi vida estará completa. Mientras tanto, Google es nuestro amigo. ","date":"2012-02-10","objectID":"/es/functional-programming-for-the-rest-of-us/:2:5","tags":["Functional","Programming","Lambda","History"],"title":"Programación funcional para el resto de nosotros","uri":"/es/functional-programming-for-the-rest-of-us/"},{"categories":["Development","Fundamentals"],"content":"Tienes algún comentario? Si tienes alguna pregunta, comentario, o sugerencia, por favor déjame una nota en coffeemug(AT)gmail.com (este es el correo electrónico de Slava Akhmechet, el autor original). Será un placer conocer tus opiniones. Ir al índice de la serie ","date":"2012-02-10","objectID":"/es/functional-programming-for-the-rest-of-us/:2:6","tags":["Functional","Programming","Lambda","History"],"title":"Programación funcional para el resto de nosotros","uri":"/es/functional-programming-for-the-rest-of-us/"},{"categories":["Development","Parallel Series"],"content":"Ir al índice de la serie ","date":"2012-02-08","objectID":"/es/luces-camara-action/:0:0","tags":["csharp","net framework","Task","Parallel","Async"],"title":"Luces, cámara... action!","uri":"/es/luces-camara-action/"},{"categories":["Development","Parallel Series"],"content":"Magia sin delegados Los delegados de tipo Action son una de las pequeñas maravillas traídas a .NET desde la programación funcional. Pueden definirse como un método que tiene un sólo parámetro (en su sobrecarga más simple) y que no devuelve ningún valor. Habitualmente suelen usarse para almacenar referencias a métodos o para pasar un método como parámetro sin tener que declarar explícitamente un delegado. Basta definir el parámetro con la misma firma que se espera recibir y la magia empieza a actuar. Un detalle importante que podemos ver al observar la firma de Action es que el tipo T es contravariante, de modo que podemos usar este tipo en cualquier otro tipo derivado. Si quieres saber más sobre covarianza y contravarianza en Generics dale un buen vistazo a este post del blog del colega Eduard Tomàs. Veamos un poco de esta magia. Suponiendo que tenemos un método que admite un parámetro de tipo Action podemos llamar al método y pasarle (o más bien inyectarle) el comportamiento deseado, es decir pasarle un método que cumpla con la firma por parámetro: void test() { string msg = \"This is the value...\"; doSomethingWithStringValue(enqueueMessage, msg); doSomethingWithStringValue(saveToDatabase, msg); doSomethingWithStringValue(writeMessageToConsole, msg); } private void doSomethingWithStringValue(Action\u003cstring\u003e actionToDo, string value) { //do several things with this value validateMessage(value); compressMessage(value); //when finishing... actionToDo(value); } private void enqueueMessage(string value) { //do something \u0026 enqueue this value Queue\u003cstring\u003e messages = new Queue\u003cstring\u003e(); messages.Enqueue(value); } private void saveToDatabase(string value) { //do something \u0026 save to db this value addLineToUserLog(value); } private void writeMessageToConsole(string value) { //do something \u0026 output this value Console.WriteLine(value); } Por un lado tenemos tres métodos que hacen cosas distintas pero tienen la misma firma (todos esperan un parámetro de tipo string). Y por el otro tenemos un método que tiene un parámetro de tipo Action. Es decir, este parámetro admite como valor cualquier método que tenga la misma firma que hemos declarado. De este modo, podemos invocarlo varias veces y en cada una de ellas de podemos decir que utilice un método distinto para hacer algo distinto. Muy similar a las funciones asíncronas de Javascript o al patrón Promise. Bonito, eh? Es lo mismo que utilizar delegados pero, uhm… espera! Si, sin usarlos 😄 ","date":"2012-02-08","objectID":"/es/luces-camara-action/:1:0","tags":["csharp","net framework","Task","Parallel","Async"],"title":"Luces, cámara... action!","uri":"/es/luces-camara-action/"},{"categories":["Development","Parallel Series"],"content":"Actions por todos lados Pues cada vez son más las clases del framework que hacen uso de este tipo de delegados y de su hermano Func, que viene a ser lo mismo pero retornando un valor. Sin ir más lejos, los métodos extensores de LINQ (Select, Where, OrderBy) utilizan Func y casi toda la TPL se basa en el uso de Action, desde los bucles For y ForEach de la clase estática Parallel, hasta la creación explícita de tareas mediante la clase Task. Por ejemplo, cuando deseamos ejecutar una tarea de forma asíncrona, podemos utilizar el método StartNew de la clase Task.Factory. Este método tiene una sobrecarga en el que acepta un parámetro de tipo Action o Func, y lo mejor de todo es que puede crearse inline (en línea), es decir en el mismo momento en que se realiza la llamada. Veamos unos ejemplos: Partiendo de un método simple: private void doSomething() { //Pause for 0 to 10 seconds (random) Random r = new Random(Guid.NewGuid().GetHashCode()); Thread.Sleep(r.Next(10000)); } Puesto que es un método que ni recibe parámetros ni devuelve nada podemos llegar a utilizar su sobrecarga más sencilla: Task.Factory.StartNew(doSomething); Otra opción, si el método tuviese un parámetro int para especificar el número de segundos (en lugar de ser aleatorio) podría ser esta: private void doSomething(int seconds) { int mseconds = seconds * 1000 Thread.Sleep(mseconds); } Task.Factory.StartNew(() =\u003e doSomething(5)); Aquí ya vemos algo más curioso. Algo que seguramente hemos observado muchas veces y utilizado antes: Una expresión lambda. Esta expresión es también algo tomado de la programación funcional, y puede leerse como: “va hacia”. En la parte izquierda de la expresión se especifican los parámetros de entrada o variables (si existen, en este caso no), y en la parte derecha la propia expresión. El caso anterior es tan simple que no tiene parámetros y sólo usamos la parte derecha de la expresión para enviar el valor 5 al método. Al usar una expresión lambda se permite que las instrucciones contenidas en dicha expresión puedan varias líneas, de modo que también podemos llegar a hacer algo como esto: Task.Factory.StartNew(() =\u003e { int x = 5; doSomething(x); Console.WriteLine(\"finished!\"); }); O directamente esto: Task.Factory.StartNew(() =\u003e { int x = 5; int mseconds = seconds * 1000 Thread.Sleep(mseconds); Console.WriteLine(\"finished!\"); }); En este caso, podemos incluso omitir el método doSomething y usar el código inline directamente en la llamada a StartNew. No obstante, un consejo: No es conveniente abusar de las expresiones inline, de modo que si tenemos más de 5 ó 6 líneas tal vez será más conveniente refactorizar este código para no hacerlo demasiado complejo y respetar los buenos principios de diseño. ","date":"2012-02-08","objectID":"/es/luces-camara-action/:2:0","tags":["csharp","net framework","Task","Parallel","Async"],"title":"Luces, cámara... action!","uri":"/es/luces-camara-action/"},{"categories":["Development","Parallel Series"],"content":"Ahora con parámetros Hasta ahora al realizar la llamada siempre hemos usado un delegado de tipo Action sin parámetros, de ahí los paréntesis vacíos en la parte izquierda de la expresión lambda. Sin embargo encontraremos multitud de casos en los que debemos pasar parámetros. Sin ir más lejos el método Parallel.For tiene un parámetro de tipo Action al que hay que pasarle un valor de tipo int, lógico por otra parte ya que dentro de un bucle es muy necesario conocer en todo momento el valor de la iteración: Parallel.For(1, 40, (i) =\u003e { serie.Add(i.Fibonacci()); }); Observar que no es necesario definir el tipo de datos de la variable i porque el propio compilador es capaz de inferirlo, pero evidentemente también podemos declarar el tipo previo al nombre de la variable, como siempre (int i). Podemos pasar tantos parámetros como necesite la Action, el mismo método tiene otra sobrecarga que admite un objeto ParallelLoopState para poder cancelar el bucle: Parallel.For(1, 40, (i, loopState) =\u003e { serie.Add(i.Fibonacci()); if (i \u003e 35) loopState.Break(); }); Y por supuesto podemos crearnos nuestras propias acciones con tantos parámetros como sean necesarios. Aunque al igual que ante, si necesitamos pasar más de 3 ó 4 parámetros a un Action tal vez deberíamos plantearnos si estamos haciendo las cosas bien private void saveToDatabase(string value, bool useDetails) { addLineToUserLog(value); if (useDetails) addLineToUserLogDetails(); } void test() { //Define una acción que apunta al método saveToDatabase Action\u003cstring, bool\u003e myAction = (v, s) =\u003e { saveToDatabase(v, s); }; string value = \"This is the value...\"; bool usedetails = true; myAction(value, usedetails); //Aquí se llama a la acción y al método al que apunta } ","date":"2012-02-08","objectID":"/es/luces-camara-action/:3:0","tags":["csharp","net framework","Task","Parallel","Async"],"title":"Luces, cámara... action!","uri":"/es/luces-camara-action/"},{"categories":["Development","Parallel Series"],"content":"Resumiendo Los delegados de tipo Action son muy útiles para simplificar el trabajo con delegados (ahora que lo pienso hace bastante tiempo que no los uso, ni para declarar eventos). Nos permiten especificar las acciones a realizar pudiendo llegar a tener hasta 16 parámetros -demasiados en mi opinión- y al igual que los método void no devuelven ningún valor. Si queremos lo mismo pero pudiendo retornar un resultado debemos utilizar su hermano Func\u003cT, TResult\u003e que es exactamente igual, pero en todas sus sobrecargas (y tiene tantas como Action) el último argumento representa el valor de retorno. Ir al índice de la serie ","date":"2012-02-08","objectID":"/es/luces-camara-action/:4:0","tags":["csharp","net framework","Task","Parallel","Async"],"title":"Luces, cámara... action!","uri":"/es/luces-camara-action/"},{"categories":["Development","Parallel Series"],"content":"Ir al índice de la serie ","date":"2011-06-26","objectID":"/es/parallelseries05-parallel-static-class/:0:0","tags":["csharp","net framework","Task","Parallel","Async"],"title":"Parallel Series 05 - Parallel Static Class","uri":"/es/parallelseries05-parallel-static-class/"},{"categories":["Development","Parallel Series"],"content":"Parallel Static Class Hoy quiero hablaros de la clase estática Parallel. Esta clase provee soporte para paralelizar bucles y regiones, y al igual que PLINQ su uso es muy sencillo. Cabe destacar que está especialmente optimizada para iteraciones, y que en este contexto se desenvuelve un poco mejor que PLINQ. No hay una diferencia significativa en tiempos absolutos, pero puede verse perfectamente si utilizamos el magnífico profiler de Visual Studio 2010. No obstante, pueden existir situaciones en las que si se necesita afinar mucho el rendimiento en iteraciones, y aquí es dónde tiene más sentido utilizar dos de los tres métodos de esta clase: For y ForEach. Al tercero lo llamaremos Cirdan y apenas aparecerá en esta historia (en realidad me refiero a Invoke pero tampoco aparecerá por aquí). ","date":"2011-06-26","objectID":"/es/parallelseries05-parallel-static-class/:1:0","tags":["csharp","net framework","Task","Parallel","Async"],"title":"Parallel Series 05 - Parallel Static Class","uri":"/es/parallelseries05-parallel-static-class/"},{"categories":["Development","Parallel Series"],"content":"Comprendiendo las acciones Los dos métodos tienen una firma muy similar en su forma más sencilla. Ambos iteran sobre una serie de instrucciones realizando n veces cada una de ellas. Y aquí es dónde vemos aparecer los parámetros de tipo Action: public static ParallelLoopResult For (int fromInclusive, int toExclusive, Action\u003cint\u003e body) public static ParallelLoopResult ForEach\u003cTSource\u003e (IEnumerable\u003cTSource\u003e source, Action\u003cTSource\u003e body) Un Action, al igual que su hermano Func es uno de los elementos de C# importados de la programación funcional, y desde el momento en que uno se acostumbra a usarlo, cuesta pensar cómo ha podido desarrollar toda su vida anterior. Si no, los que estéis acostumbrados a usar expresiones lambda en LINQ, imagináos que desaparecen de un día para otro. No quiero empezar a divagar ahora sobre programación funcional, aunque si que quiero hacer incapié en el uso de Actions y lo importantes que se han vuelto en los últimos años. De hecho, recientemente he dedicado un post a cómo Action y Func han simplificado mucho el trabajo con delegados a los desarrolladores. ","date":"2011-06-26","objectID":"/es/parallelseries05-parallel-static-class/:1:1","tags":["csharp","net framework","Task","Parallel","Async"],"title":"Parallel Series 05 - Parallel Static Class","uri":"/es/parallelseries05-parallel-static-class/"},{"categories":["Development","Parallel Series"],"content":"El método Parallel.For Pero volviendo al tema que nos ocupa, si observamos la firma del método Parallel.For podremos ver que en lo importante no difiere demasiado de su homólogo for de toda la vida: Ambos tienen un inicio, un final y unas acciones a realizar un número determinado de veces. Así que partiendo del método IsPrime que ya utilizamos en el anterior post sobre PLINQ, vamos a ver una comparativa entre las sintaxis de éstos dos métodos: for (int i = 0; i \u003c 100; i++) { if(i.IsPrime()) Console.WriteLine(string.Format(\"{0} es primo\", i)); else Console.WriteLine(string.Format(\"{0} no es primo\", i)); } Parallel.For(0, 100, (i) =\u003e { if (i.IsPrime()) Console.WriteLine(string.Format(\"{0} es primo\", i)); else Console.WriteLine(string.Format(\"{0} no es primo\", i)); }); En ambos casos tenemos una serie de líneas que deben ejecutarse 100 veces. Concretamente desde 0 hasta 99, ya que el elemento superior no se incluye en ninguno de los dos casos. Sólo se ve un poco extraño el uso del Action, pero podéis pensar en que la variable int i del primer bucle for, aquí se transforma en la parte (i) a la izquierda de la expresión lambda (=\u003e). Y las acciones a ejecutar del primer for son exactamente iguales y van a la derecha de la expresión lambda. ","date":"2011-06-26","objectID":"/es/parallelseries05-parallel-static-class/:1:2","tags":["csharp","net framework","Task","Parallel","Async"],"title":"Parallel Series 05 - Parallel Static Class","uri":"/es/parallelseries05-parallel-static-class/"},{"categories":["Development","Parallel Series"],"content":"So, let’s parallelize! Viéndolo de este modo debe resultar extremadamente sencillo transformar todos nuestros bucles de este modo, así que ¿debemos hacerlo? La respuesta es NO. En algunas ocasiones no vamos a obtener rendimiento por el hecho de paralelizar, ya que si el trabajo a realizar es mínimo, tardaremos más tiempo en dividir el trabajo en distintos threads, ejecutarlos y consolidar la información que en ejecutar la tarea sin paralelizar. También podría ser que nos encontrásemos un cuello de botella externo en un dispositivo de I/O, como un puerto, un servidor remoto o un socket. Otro claro ejemplo de esto son los bucles anidados. Es común anidar varias estructuras for o foreach para realizar ciertos algoritmos. En este caso el candidato a ser paralelizado siempre es el bucle exterior y no es necesario (de hecho sería contraproducente) paralelizar los bucles internos: Parallel.For(0, 100, (z) =\u003e { for (int i = 0; i \u003c 100; i++) { if (i.IsPrime()) Console.WriteLine(string.Format(\"{0} es primo\", i)); else Console.WriteLine(string.Format(\"{0} no es primo\", i)); } }); Por lo pronto resulta bastante evidente, ya que si paralelizamos en bucle exterior necesitaríamos un ordenador con 100 cores y evidentementemente todavía no existen, así que la TPL tiene que agrupar estas tareas para adaptarlas a los cores disponibles, tardando cierto tiempo en hacer la sincronización (parecido a los primeros ejemplos con monos de la serie). Imagináos entonces si paralelizamos ambos bucles: 100 x 100 = 10.000 cores? Simplemente no tiene sentido. Mi consejo es que en todos los casos en los que se decida paralelizar un bucle (y esto también vale para las consultas PLINQ) se realice primero una comparativa de rendimiento. Tu amigo el profiler no engaña ;) ","date":"2011-06-26","objectID":"/es/parallelseries05-parallel-static-class/:1:3","tags":["csharp","net framework","Task","Parallel","Async"],"title":"Parallel Series 05 - Parallel Static Class","uri":"/es/parallelseries05-parallel-static-class/"},{"categories":["Development","Parallel Series"],"content":"El método Parallel.ForEach En cuanto al método ForEach es prácticamente igual al anterior con la salvedad que no tenemos un inicio y un final, sino una secuencia de entrada de datos (basada en IEnumerable, como PLINQ) y una variable que usamos para iterar por cada uno de los elementos de la secuencia y realizar una serie de acciones. Consideremos el siguiente código: List\u003cFeedDefinition\u003e feeds = new List\u003cFeedDefinition\u003e(); clock.Restart(); var blogs = FeedsEngine.GetBlogsUrls(); foreach (var blog in blogs) { feeds.AddRange(FeedsEngine.GetBlogFeeds(blog)); } clock.Stop(); this.Text = clock.ElapsedMilliseconds.ToString(\"n2\"); feeds.ForEach(p =\u003e Console.WriteLine(p.Name)); Suponiendo que tenemos un método FeedsEngine.GetBlogsUrls que devuelve una lista de urls de proporcionan contenido RSS, el código anterior se conecta a cada una de las urls e intenta descargar toda la información de los posts mediante un método FeedsEngine.GetBlogFeeds(blog). Nota: El código completo lo podréis encontrar en el post (todavía no publicado) ‘Código de ejemplo de las Parallel Series’, que contiene todos los ejemplos de todos los posts de la serie. Como podéis imaginar este proceso totalmente secuencial es un serio candidato a ser paralelizado, ya que la mayoría del tiempo de este proceso es tiempo desperdiciado intentando a conectar con un servidor externo y que éste responda a las peticiones. En este caso paralelizar va a ser de gran ayuda aunque es importante comprender que en este caso la ganancia de rendimiento no va a ser por usar más potencia local, sino por lanzar las peticiones a los distintos servidores de forma asíncrona. Así pues, basta cambiar la parte del bucle foreach por su versión paralelizada: Parallel.ForEach(blogs, (blog) =\u003e { feeds.AddRange(FeedsEngine.GetBlogFeeds(blog)); }); En la que definimos la secuencia de datos a utilizar y declaramos la variable blog al vuelo (el compilador infiere el tipo automáticamente) a la izquierda de la expresión lambda, y a la derecha las acciones que deseamos realizar, que son exactamente iguales a la anterior versión foreach. Y comprobaremos como se ejecuta mucho más rápido. En mi estación de trabajo pasamos de 6,7 segundos a 1,4 lo que no está nada mal. ","date":"2011-06-26","objectID":"/es/parallelseries05-parallel-static-class/:1:4","tags":["csharp","net framework","Task","Parallel","Async"],"title":"Parallel Series 05 - Parallel Static Class","uri":"/es/parallelseries05-parallel-static-class/"},{"categories":["Development","Parallel Series"],"content":"Explorando más opciones En la clase Parallel al igual que en las consultas PLINQ, existe la posibilidad de especificar el grado de paralelismo así como de cancelar la ejecución de un bucle. Sólo debemos usar una de las sobrecargas que utiliza un objeto de tipo ParallelOptions. private void button11_Click(object sender, EventArgs e) { CancellationTokenSource cs = new CancellationTokenSource(); var cores = Environment.ProcessorCount; clock.Restart(); var options = new ParallelOptions() { MaxDegreeOfParallelism = cores / 2, CancellationToken= cs.Token }; try { Parallel.For(1, 10, options, (i) =\u003e { dowork_cancel(i, cs); }); } catch (Exception ex) { MessageBox.Show(ex.Message); } clock.Stop(); this.Text = clock.ElapsedMilliseconds.ToString(\"n2\"); } void dowork_cancel(int i, CancellationTokenSource cs) { Thread.Sleep(1000); if (i == 5) cs.Cancel(); } En el caso anterior especificamos un grado de paralelización de la mitad del número de cores y preparemos la consulta para su posible cancelación (algo que simulamos en el interior del método dowork_cancel al llegar el contador a 5). A continución… En el próximo post veremos cómo utilizar la clase estática Parallel, optimizada para trabajar con procesos iterativos, esos típicos bucles que todas las aplicaciones tienen. Ir al índice de la serie ","date":"2011-06-26","objectID":"/es/parallelseries05-parallel-static-class/:1:5","tags":["csharp","net framework","Task","Parallel","Async"],"title":"Parallel Series 05 - Parallel Static Class","uri":"/es/parallelseries05-parallel-static-class/"},{"categories":["Development","Parallel Series"],"content":"Ir al índice de la serie ","date":"2011-05-31","objectID":"/es/parallelseries04-plinq/:0:0","tags":["csharp","net framework","Task","Parallel","Async"],"title":"Parallel Series 04 - PLINQ","uri":"/es/parallelseries04-plinq/"},{"categories":["Development","Parallel Series"],"content":"LINQ power! Creo que estaremos todos de acuerdo en que LINQ ha supuesto una revolución en la forma de desarrollar, y ha hecho que muchos desarrolladores de otros lenguajes nos miren con cierto tono de envidia… E incluso que otras plataformas estén haciendo serios esfuerzos para incorporarlo en sus Frameworks :-) Ahora, con la llegada de la Task Parallel Library, se abre un mundo de posibilidades gracias a PLINQ, que permite -de forma extremadamente sencilla- convertir cualquier consulta LINQ secuencial en una consulta paralelizable, permitiendo su segmentación y ejecución en los distintos cores en paralelo. Es decir cualquier consulta LINQ, cómo la siguiente, en la que tenemos un array llamado numbers y un método IsPrime que devuelve un valor boolean en función de si un número es primo. Suponiendo esta función que devuelve si un número es primo: public static bool IsPrime(this int n) //1 = false, 2 = true, 3 = true... { if (n \u003c= 1) return false; if ((n \u0026 1) == 0) { if (n == 2) return true; else return false; } for (int i = 3; (i * i) \u003c= n; i += 2) { if ((n % i) == 0) return false; } return n != 1; } Puede ser ejecutada con LINQ de este modo para que retorne sólo aquellos números que son primos: var query = from n in numbers where n.IsPrime() select n; Y ésta ser paralelizada simplemente agregando AsParallel de este modo: var query = from n in numbers.AsParallel() where n.IsPrime() select n; Partiendo de que el array de números contiene los primeros 10 millones de números enteros, y de que mi estación de trabajo actual tiene un procesador i7 con 8 cores, el resultado es abrumador: La consulta LINQ tarda 5,2 segundos frente a los 1,3 segundos de la segunda. Es decir, casi 4 segundos menos o un 400% más rápido. Nada mal, eh? ","date":"2011-05-31","objectID":"/es/parallelseries04-plinq/:1:0","tags":["csharp","net framework","Task","Parallel","Async"],"title":"Parallel Series 04 - PLINQ","uri":"/es/parallelseries04-plinq/"},{"categories":["Development","Parallel Series"],"content":"¿Dónde está la magia? La magia verdadera es que PLINQ nos abstrae de todo el proceso de paralelización, creación de tareas, threads, sincronización y consolidación de los datos. Y además creo que lo hace de forma muy elegante :-) Como ya sabemos, las consultas LINQ se basan en IEnumerable (gracias Generics!) que expone un enumerador para recorrer los elementos de una secuencia de elementos de tipo T. Esto hace que todas las colecciones que puedan devolverse en este tipo de consultas (IOrderedEnumerable, IQueryable, etc.) implementen esta interfaz. Hasta aquí nada nuevo bajo el sol. Sin embargo, en la consulta PLINQ al utilizar el método extensor AsParallel() estamos transformando la secuencia de entrada de IEnumerable a ParallelQuery permitiendo la segmentación de los elementos de la secuencia y ejecutando cada uno de los segmentos en un thread distinto. Y por supuesto, repartiendo el trabajo en los diversos cores (si los hay). Segmentación en paralelo de la secuencia La secuencia de entrada se particiona y se manda por fragmentos a distintos threads que invocan al método IsPrime devolviendo true (T) o false (F), y posteriormente los consolida en una secuencia de salida que puede ser consumida. No obstante, el hecho de paralelizar el trabajo no garantiza que el resultado sea devuelto en el mismo orden, ya que es posible que un thread termine antes que otro y devuelva su resultado parcial antes de lo esperado. Así que, si la ordenación de los datos de salida es importante tenemos que ir un paso más allá. Los primeros elementos deberían ser 2, 3, 5, 7… no 59 y 71 ¿? ","date":"2011-05-31","objectID":"/es/parallelseries04-plinq/:1:1","tags":["csharp","net framework","Task","Parallel","Async"],"title":"Parallel Series 04 - PLINQ","uri":"/es/parallelseries04-plinq/"},{"categories":["Development","Parallel Series"],"content":"PLINQ y la ordenación Para asegurar la ordenación del conjunto de resultados, basta agregar el método AsOrdered() a la consulta. Este método asegura la correcta ordenación, a costa de implementar una serie de mecanismos de sincronización. Estos mecanismos, lógicamente retardan un poco el tiempo de entrega de los resultados, pero es despreciable. En mi estación de trabajo se arrojan unos valores de 1,311 segundos sin ordenar frente a 1,344 segundos ordenados (apenas 30 milésimas). Estos resultados son la media de una serie de 50 mediciones, con lo que son bastante fiables. Una vez modificada la consulta: var query = from n in numbers.AsParallel().AsOrdered() where n.IsPrime() select n; Ahora si están ordenados :) ","date":"2011-05-31","objectID":"/es/parallelseries04-plinq/:1:2","tags":["csharp","net framework","Task","Parallel","Async"],"title":"Parallel Series 04 - PLINQ","uri":"/es/parallelseries04-plinq/"},{"categories":["Development","Parallel Series"],"content":"Especificar el grado de paralelización En la mayoría de las charlas que he dado sobre la TPL se acostumbra a preguntar respecto a funciones que acceden a recursos externos (servicios, sockets, etc.). En estos casos aparece claramente un cuello de botella, y no porque una función necesite hacer uso intensivo de la CPU, sino porque debe esperar un resultado externo. Aquí suele ser interesante especificar el grado de paralelización de deseamos. Otro caso interesante para especificar el grado de paralelización puede ser el típico escenario de productor/consumidor. Es interesante notar que al especificar el grado de paralelización no estamos forzando a que se usen n particiones, sino que simplemente estamos especificando el valor máximo: var cores = Environment.ProcessorCount; var query = from n in numbers.AsParallel().AsOrdered(). WithDegreeOfParallelism(cores / 2) where n.IsPrime() select n; De este modo, al definir el grado de paralelización en la mitad del número de cores del procesador nos aseguramos que (por ejemplo) podremos tener un hilo que vaya creando elementos (productor) y otro hilo que vaya consumiendo dichos elementos (consumidor). ","date":"2011-05-31","objectID":"/es/parallelseries04-plinq/:1:3","tags":["csharp","net framework","Task","Parallel","Async"],"title":"Parallel Series 04 - PLINQ","uri":"/es/parallelseries04-plinq/"},{"categories":["Development","Parallel Series"],"content":"Cancelación de una consulta PLINQ En ocasiones, una consulta PLINQ puede ser cancelada. Bien porque durante el proceso se ha encontrado un error y ya no es necesario terminar de calcular el resto de resultados, o simplemente porque ha sido cancelada por el usuario. Es estos casos, es necesario utilizar un token de cancelación. Este token tiene su origen en la estructura CancellationTokenSource, que representa ‘una potencial cancelación’ y proporciona los mecanismos para cancelar y comprobar el estado de una tarea asíncrona, de modo que puede utilizarse con todos los elementos de la Task Parallel Library, no sólo con PLINQ. A continuación, vamos a modificar el código del ejemplo que hemos usado hasta ahora para simular un error y comprobar el funcionamiento de la cancelación de tareas en PLINQ. Para ello lo primero que vamos a hacer es crear una sobrecarga del método IsPrime, que reciba un parámetro de tipo CancellationTokenSource, para poder cancelar la tarea: public static bool IsPrime(this int n, CancellationTokenSource cs) { if (n == 1000) cs.Cancel(); return IsPrime(n); } A modo de ejemplo, cuando el número a calcular sea 1.000 cancelaremos la tarea, de modo que no sea necesario llegar a los 10 millones. De este modo, por un lado se lanzará una excepción y por otro el tiempo en ejecutar la consulta PLINQ será mucho menor. private void plinq_cancellable() { var numbers = Enumerable.Range(1, 10000000); using (var cs = new CancellationTokenSource()) { var clock = Stopwatch.StartNew(); var query = numbers.AsParallel().AsOrdered(). WithCancellation(cs.Token). Where(p =\u003e p.IsPrime(cs)); try { var result = query.ToArray(); } catch (OperationCanceledException ex) { Console.WriteLine(ex.Message); } clock.Stop(); this.Text = clock.ElapsedMilliseconds.ToString(\"n2\"); } } Por un lado tenemos que tener la precaución de envolver la consulta dentro de un bloque try-catch (en este caso sólo la llamada a ToArray() que es realmente cuando se ejecuta la consulta), y por el otro especificamos que la consulta puede ser cancelada mediante WithCancellation. A continuación creamos un objeto de tipo CancellationTokenSource para administrar la cancelación de esta consulta. Este objeto será el que finalmente pasemos al método IsPrime() y en caso que se cancele provocará que su propiedad IsCancellationRequested devuelva true y que se produzca una bonita excepción de tipo OperationCanceledException. Al llegar a 1000... boom! ","date":"2011-05-31","objectID":"/es/parallelseries04-plinq/:1:4","tags":["csharp","net framework","Task","Parallel","Async"],"title":"Parallel Series 04 - PLINQ","uri":"/es/parallelseries04-plinq/"},{"categories":["Development","Parallel Series"],"content":"Limitaciones de PLINQ No quiero extenderme mucho más porque creo que hay material suficiente para hacer un post más adelante sobre temas avanzados. Sin embargo quiero dejar claro que existen algunas limitaciones en PLINQ, como el uso de algunos operadores (Take, SkipWhile) y de las versiones indexadas de Select o ElementAt. Además existen otros casos en los que por cuestiones de rendimiento no es recomendable usar PLINQ en todos los casos, debido al sobrecoste que puede llegar a ocasionar, como el uso de Join, Union o GroupBy. Sin embargo, trataremos éstas cuestiones más adelante. Próximamente veremos cómo utilizar la clase estática Parallel, optimizada para trabajar con procesos iterativos, esos típicos bucles que todas las aplicaciones tienen. ","date":"2011-05-31","objectID":"/es/parallelseries04-plinq/:1:5","tags":["csharp","net framework","Task","Parallel","Async"],"title":"Parallel Series 04 - PLINQ","uri":"/es/parallelseries04-plinq/"},{"categories":["Development","Parallel Series"],"content":"Video Aquí tienes un vídeo corto (15 minutos) sobre lo que acabamos de contar ;) A continución… En el próximo post veremos cómo utilizar la clase estática Parallel, optimizada para trabajar con procesos iterativos, esos típicos bucles que todas las aplicaciones tienen. Ir al índice de la serie ","date":"2011-05-31","objectID":"/es/parallelseries04-plinq/:2:0","tags":["csharp","net framework","Task","Parallel","Async"],"title":"Parallel Series 04 - PLINQ","uri":"/es/parallelseries04-plinq/"},{"categories":["Development","Parallel Series"],"content":"Ir al índice de la serie ","date":"2011-03-03","objectID":"/es/parallelseries03-conceptos-base/:0:0","tags":["csharp","net framework","Task","Parallel","Async"],"title":"Parallel Series 03 - Conceptos base","uri":"/es/parallelseries03-conceptos-base/"},{"categories":["Development","Parallel Series"],"content":"Elementos del sistema opertivo Cuando hablamos de programación paralela conviene tener bastante claros algunos conceptos a nivel de sistema operativo. En este apartado trataremos de aclarar estos términos, ya que más adelante los usaremos frecuentemente. ","date":"2011-03-03","objectID":"/es/parallelseries03-conceptos-base/:1:0","tags":["csharp","net framework","Task","Parallel","Async"],"title":"Parallel Series 03 - Conceptos base","uri":"/es/parallelseries03-conceptos-base/"},{"categories":["Development","Parallel Series"],"content":"Procesos (Processes) Toda aplicación ejecutándose en el sistema operativo existe dentro del contexto de un proceso, aunque no todos los procesos se corresponden con aplicaciones visibles. Basta abrir el administrador de tareas para comprobar que la lista de procesos es bastante mayor a la de aplicaciones. Eso es así porque pueden corresponderse a servicios, aplicaciones no visibles o porque algunas aplicaciones están diseñadas para crear varios procesos (hola +ponga aquí el nombre de su explorador favorito+ 😄). Procesos de Windows en el Administrador de Tareas Un proceso proporciona los recursos necesarios para ejecutar un programa. Contiene un espacio de memoria virtual, código ejecutable, un contexto de seguridad, un identificador de proceso único, variables de entorno, y al menos un thread de ejecución. Un proceso de Windows Cada proceso se inicia con un único hilo (thread en adelante), a menudo llamado thread principal. Pero puede crear threads adicionales, que pueden ser utilizados para encargarse de diferentes tareas. Hacer llamadas entre procesos resulta complejo y muy costoso en términos de rendimiento debido a que deben usarse mecanismos especiales como pipes, sockets o llamadas RPC (Remote procedure call). ","date":"2011-03-03","objectID":"/es/parallelseries03-conceptos-base/:1:1","tags":["csharp","net framework","Task","Parallel","Async"],"title":"Parallel Series 03 - Conceptos base","uri":"/es/parallelseries03-conceptos-base/"},{"categories":["Development","Parallel Series"],"content":"Dominios de aplicación .NET (AppDomains) Al ser .NET una plataforma que ejecuta código administrado, los procesos que se crean al ejecutar éstas aplicaciones son un poco diferentes, ya que cuando se diseñó .NET una de las máximas preocupaciones fue la de tratar de mejorar el manejo de los procesos clásico o no administrados. Por ello se creó el concepto de dominio de aplicación, que podría definirse como un proceso lógico dentro del proceso del sistema operativo. Procesos en .NET Framework La gran diferencia es que dentro de un proceso podemos crear distintos dominios de aplicación y cargar en cada uno de ellos varios ensamblados, y aprovechar que las llamadas entre distintos dominios de aplicación y los ensamblados que contienen son mucho más rápidas que entre procesos. Si uno de estos ensamblado debe ser compartido entre dos dominios de aplicaciones éste se copia en cada uno de los dominios. De este modo, al usar los dominios de aplicación se obtiene la ventaja de aislar el código de un proceso a otro, pero sin pagar el sobrecoste dedicado a realizar llamadas entre procesos. ","date":"2011-03-03","objectID":"/es/parallelseries03-conceptos-base/:1:2","tags":["csharp","net framework","Task","Parallel","Async"],"title":"Parallel Series 03 - Conceptos base","uri":"/es/parallelseries03-conceptos-base/"},{"categories":["Development","Parallel Series"],"content":"Hilos (Threads) Un thread es la entidad dentro de un proceso encargada de ejecutar el código. Todos los threads que contiene un proceso comparten los recursos y memoria virtual de éste, y mantienen controladores de excepciones, una prioridad de programación, almacenamiento local, y un identificador de thread único. Los threads son independientes a los dominios de aplicación, de forma que podemos pensar en ellos como elementos transversales que pueden saltar de un uno a otro a lo largo del tiempo. No existe ninguna correspondencia entre el número de threads y de dominios de aplicación. Por defecto, todos los procesos se crean con un thread por defecto llamado thread principal, aunque en las aplicaciones .NET se crean al menos dos, ya que es necesario un segundo thread para administrar el recolector de basura. No obstante cada proceso puede crear un número casi ilimitado de ellos, aunque en última instancia el sistema operativo siempre tiene la potestad de priorizar a la baja estos hilos o incluso congelarlos. Threads dentro de un proceso Realizar cambios de contexto entre threads es muchísimo más rápido que los cambios de contexto de proceso. De hecho en los sistemas operativos que utilizan multitarea preemptiva (la gran mayoría hoy en día) el sistema operativo va cediendo una pequeña fracción de tiempo a cada uno de los threads de cada uno de los procesos cargados para que ejecuten una porción de su código ejecutable, dando la sensación de que varias aplicaciones se ejecutan al mismo tiempo. Este tipo de multitarea tiene la ventaja frente a sus predecesores de que si un proceso deja de responder, el sistema no se colapsa y puede seguir respondiendo sin verse afectado por la caída del mismo. Esto en la práctica ha significado la casi desaparición de las llamadas BSOD (Blue Screen Of Death) causadas por este motivo. ","date":"2011-03-03","objectID":"/es/parallelseries03-conceptos-base/:1:3","tags":["csharp","net framework","Task","Parallel","Async"],"title":"Parallel Series 03 - Conceptos base","uri":"/es/parallelseries03-conceptos-base/"},{"categories":["Development","Parallel Series"],"content":"Multihilo (Multithreading) A la capacidad que tienen los procesos de crear distintos threads ejecutándose simultáneamente es a lo que llamamos Multithreading. Y nos ha permitido simular la multitarea en los ordenadores personales de la última década y media. Esto es así porque aunque físicamente sólo haya un microprocesador, en términos del sistema operativo éste cede un periodo de tiempo a cada thread de cada uno de los procesos cargados en el sistema, y al repetirse una y otra muy rápidaamente vez produce la sensación de que todas las aplicaciones se ejecutan al mismo tiempo, pero nada más lejos de la realidad. Al menos hasta hace poco. Threads ejecutándose en un core de procesdor ","date":"2011-03-03","objectID":"/es/parallelseries03-conceptos-base/:1:4","tags":["csharp","net framework","Task","Parallel","Async"],"title":"Parallel Series 03 - Conceptos base","uri":"/es/parallelseries03-conceptos-base/"},{"categories":["Development","Parallel Series"],"content":"Paralelismo (Parallelism) Con la aparición de los primeros microprocesadores con varios núcleos (cores), al fin se pudo ejecutar código en paralelo y obtener la tan deseada multitarea real, ya que distintos threads pueden ejecutarse en distintos cores al mismo tiempo. De modo que a más cores, más threads pueden ejecutarse y por consiguiente más código al mismo tiempo. Threads ejecutándose en varios cores a la vez Hace apenas cuatro o cinco años de la aparición de los primeros dual core, sólo un poco más tarde aparecieron los quad core, y hoy en día es bastante habitual ver estaciones de trabajo con 8 y hasta 16 cores. En cuanto al futuro, nadie sabe al ritmo que evolucionará esta tecnología, pero los chicos de Intel hace más de dos años ya filtraron imágenes de un Windows Server con 256 cores. Incluso los súper ligeros procesadores para teléfonos y tablets basados en arquitectura ARM están empezando a lanzar modelos de dos y cuatro cores. Quién quiere uno de estos? Volviendo al ejemplo de los monos, resulta muy tentador pensar que si aprovechamos toda la potencia de los nuevos núcleos podemos obtener ganancias de rendimiento espectaculares y escribir los 200 tomos en el tiempo que escribimos uno de ellos. Evidentemente esta afirmación es un poco exagerada, ya que siempre va a haber un arduo trabajo de sincronización entre los diferentes monos… perdón threads. Con todo, la ganancia es espectacular, llegando fácilmente a multiplicar x5 o x6 en un entorno con 8 cores. Algo nada despreciable en según qué procesos. De modo que viendo el número de cores hacia dónde nos movemos, a mi juicio se hace imperativo conocer -si no dominar- la TPL. ","date":"2011-03-03","objectID":"/es/parallelseries03-conceptos-base/:1:5","tags":["csharp","net framework","Task","Parallel","Async"],"title":"Parallel Series 03 - Conceptos base","uri":"/es/parallelseries03-conceptos-base/"},{"categories":["Development","Parallel Series"],"content":"Antes de terminar, un consejo Como hemos visto, por el momento sólo puede existir paralelismo real en una estación con varios cores. De otro modo el código funcionará sin errores pero solo utilizando el único core de la máquina. Así que un error bastante común entre los desarrolladores es utilizar máquinas virtuales para desarrollar, y olvidarnos que la mayoría no permiten especificar varios cores. De modo que es bastante probable que alguna vez nos encontremos refunfuñando porque un código bien escrito no obtiene ninguna ganancia cuando lo ejecutemos dentro de una maquina virtual 😠 ","date":"2011-03-03","objectID":"/es/parallelseries03-conceptos-base/:2:0","tags":["csharp","net framework","Task","Parallel","Async"],"title":"Parallel Series 03 - Conceptos base","uri":"/es/parallelseries03-conceptos-base/"},{"categories":["Development","Parallel Series"],"content":"Video Aquí tienes un vídeo corto (15 minutos) sobre lo que acabamos de contar ;) A continución… En el próximo post veremos cómo extender LINQ con Paralel LINQ, y de este modo dotar a nuestras consultas sobre listas enumerables de paralelismo, sin apenas impacto en el código actual. Ir al índice de la serie ","date":"2011-03-03","objectID":"/es/parallelseries03-conceptos-base/:3:0","tags":["csharp","net framework","Task","Parallel","Async"],"title":"Parallel Series 03 - Conceptos base","uri":"/es/parallelseries03-conceptos-base/"},{"categories":["Development","Parallel Series"],"content":"Ir al índice de la serie ","date":"2011-01-21","objectID":"/es/parallelseries02-un-poco-de-historia/:0:0","tags":["csharp","net framework","Task","Parallel","Async"],"title":"Parallel Series 02 - Un poco de historia","uri":"/es/parallelseries02-un-poco-de-historia/"},{"categories":["Development","Parallel Series"],"content":"Un poco de historia La programación paralela no es nada nuevo. Ya estaba presente allá en mis tiempos de estudiante hace más de 20 años 😢 y hoy en día, desde la aparición del .Net Framework 4.0 está más viva que nunca gracias a la Task Parallel Library o TPL. No obstante, decir que la TPL sólo sirve para realizar tareas asíncronas es como decir que un smartphone sólo sirve para llamar por teléfono. Es más, muchísimo más. Y es precisamente, de la mano de ésta librería que vamos a introducirnos en el apasionante mundo de la programación paralela. Esta disciplina siempre ha estado tradicionalmente asociada a los perfiles técnicos más elevados y reservada para ocasiones especiales. Sin embargo a partir de ahora y gracias a la TPL va a ser accesible a todo tipo de desarrolladores, y se va a convertir en algo muy importante, algo que todo buen desarrollador deberá añadir a su lista de activos. De hecho, va a ser una parte esencial en el futuro inmediato del desarrollo de aplicaciones a todos los niveles. Pero ¿qué realmente es la programación paralela? Podemos pensar en ella como en la posibilidad de dividir una tarea larga y pesada en varias tareas más cortas, y ejecutar éstas al mismo tiempo, de modo que tarde mucho menos que la tarea original. ","date":"2011-01-21","objectID":"/es/parallelseries02-un-poco-de-historia/:1:0","tags":["csharp","net framework","Task","Parallel","Async"],"title":"Parallel Series 02 - Un poco de historia","uri":"/es/parallelseries02-un-poco-de-historia/"},{"categories":["Development","Parallel Series"],"content":"Enciclopedias y monos Supongamos que tenemos que copiar los 200 tomos de la gran enciclopedia galáctica de Términus. Lógicamente no es lo mismo copiarlos uno tras otro, que contratar a 200 monos mutantes entrenados para copiar, y sentarlos en 200 escritorios con sus 200 bolígrafos a copiar los 200 libros. Es evidente que -de ser posible- la segunda opción sería mucho más rápida. A más recursos (escritorios y bolígrafos) más rápido terminaremos la tarea Pero ¿qué sucede si sólo tenemos 100 escritorios y bolígrafos? Pues que los monos van a tener que hacer cola y esperar su turno, de modo que cuando uno de los monos termine o se canse de escribir, deberá ceder su turno al mono que espera, provocando por el camino algunas colas y enfados por parte de los monos, que son buenos trabajadores pero un poco particulares. Con todo, a menos que estalle una guerra siempre será más rápido que la primera opción, pero eso nos deja ya la primera conclusión: a más recursos (escritorios y bolígrafos) más rápido terminaremos la tarea. Y de paso vamos a tener que preocuparnos menos por gestionar los turnos y las esperas de los monos, con todo lo que conlleva. Porque como veremos más adelante, en muchas ocasiones cuando trabajamos con monos o con threads el tiempo de sincronización es primordial, y puede marcar la diferencia entre el éxito y el fracaso de nuestra aplicación. Monos programando ","date":"2011-01-21","objectID":"/es/parallelseries02-un-poco-de-historia/:2:0","tags":["csharp","net framework","Task","Parallel","Async"],"title":"Parallel Series 02 - Un poco de historia","uri":"/es/parallelseries02-un-poco-de-historia/"},{"categories":["Development","Parallel Series"],"content":"Leyes muertas y física cuántica Sin embargo, hasta ahora nos ha ido muy bien con la programación tradicional así que ¿porque parece ser que ahora es cuestión de vida o muerte aprender este nuevo paradigma? Es decir, habitualmente hasta ahora no habían demasiadas ocasiones en las que una aplicación debía recurrir a la asincronía o al paralelismo (que como veremos más adelante no es exactamente lo mismo). Por qué ahora sí? La materia presenta efectos cuánticos que harían necesaria una tecnología diferente para seguir realizando cálculos a ese nivel La respuesta es sencilla, parece que la buena vida se termina. Si ha habido una constante en IT en los últimos 45 años, esa es la que describe la ley de Moore: En 1965 Gordon Moore, uno de los fundadores de Intel predijo que cada 2 años (18 meses al principio) se doblaría el número de componentes de un circuito integrado. Y se ha cumplido a rajatabla hasta hoy, aunque en los últimos años se están alcanzando ciertos límites que hacen que dicha ley no pueda seguir cumpliéndose. Ley de moore Simplificando un poco podríamos decir que existen un par de problemas: El escalado de frecuencia de los microprocesadores y el calor generado por los mismos. El primero de ellos hace referencia a la dificultad de seguir incrementando la velocidad de los microprocesadores, debido principalmente a que la tecnología utilizada para diseñarlos está actualmente cerca de los 32 nanómetros y el límite físico antes de que la materia experimente cambios, se calcula que está entre los 22 y los 18 nanómetros. Esta previsto alcanzar este límite aproximadamente en sólo dos o tres años, hacia 2014. A la vuelta de la esquina. Una vez alcanzado ese nivel de miniaturización, en palabras del científico Stephen Hawking: “La materia presenta efectos cuánticos que harían necesaria una tecnología diferente para seguir realizando cálculos a ese nivel”. El segundo de los problemas va ligado al primero, y es que en los últimos años el calor generado por los microprocesadores se ha ido incrementado exponencialmente, y en términos de densidad de potencia ya es igual al calor generado por la tobera de un cohete. Lo peor de todo ello es que incrementar la frecuencia sólo entre un 5 y 10 por ciento cada año, tiene un coste de casi doblar la temperatura. El futuro cercano Con esto no quiero decir que no puedan fabricarse ordenadores más rápidos en un futuro. Quiero decir que si estas predicciones son acertadas, no podrán fabricarse microprocesadores más rápidos con la tecnología actual. Tal vez sea posible si se descubre cómo construir ordenadores que utilicen tecnología óptica, nano-ingeniería para crear transistores basados en nanotubos que aprovechen el llamado efecto túnel, o cualquier otro concepto aún por descubrir. Pero por el momento no podemos contar con ello. ","date":"2011-01-21","objectID":"/es/parallelseries02-un-poco-de-historia/:3:0","tags":["csharp","net framework","Task","Parallel","Async"],"title":"Parallel Series 02 - Un poco de historia","uri":"/es/parallelseries02-un-poco-de-historia/"},{"categories":["Development","Parallel Series"],"content":"Deus ex machina Si por algo se ha caracterizado el ser humano es por su gran habilidad en resolver problemas (dejando aparte su nada desdeñable habilidad para provocarlos), de modo que ya hace unos años que se ha empezado a desarrollar y fabricar una de las soluciones a este problema. De hecho hoy en día se ha convertido en algo casi cotidiano: Se trata de fabricar procesadores con varios núcleos, que se repartan el trabajo -como los monos- consiguiendo así aumentar la velocidad. No por el hecho de ser cada vez más rápidos, si no por existir cada vez más recursos trabajando al mismo tiempo. Algo parecido -salvando las distancias- al cerebro humano, que en comparación con un ordenador es bastante más lento, pero su capacidad de procesamiento en paralelo gracias a sus millones de conexiones entre neuronas, no tiene rival con ningún otro elemento conocido en la naturaleza ni creado por el hombre. Que nos depara el futuro? A continución… En el próximo post aclararemos algunos conceptos básicos aunque necesarios cuando desarrollamos aplicaciones que hagan uso de la programación asíncrona. Ir al índice de la serie ","date":"2011-01-21","objectID":"/es/parallelseries02-un-poco-de-historia/:4:0","tags":["csharp","net framework","Task","Parallel","Async"],"title":"Parallel Series 02 - Un poco de historia","uri":"/es/parallelseries02-un-poco-de-historia/"},{"categories":["Development","Parallel Series"],"content":"Ir al índice de la serie ","date":"2011-01-11","objectID":"/es/parallelseries01-el-alfa/:0:0","tags":["csharp","net framework","Task","Parallel","Async"],"title":"Parallel Series 01 - El Alfa","uri":"/es/parallelseries01-el-alfa/"},{"categories":["Development","Parallel Series"],"content":"Prologo Cada versión del .NET framework nos sorprende con una serie de novedades, y para cada uno de nosotros hay -al menos- una que nos roba el corazón. A mi me sucedió con la aparición de Generics con el framework 2.0, los métodos extensores y LINQ en las versiones 3.0 y 3.5 y me ha pasado de nuevo con la Task Parallel Library en la versión 4.0. Pensándolo detenidamente tampoco no es tan extraño, al fin y al cabo siempre me ha gustado la programación asíncrona (algo que si alguien le llega a contar a alguno de mis primeros maestros de tecnología digital, se habría muerto de la risa). Sin embargo, con los años éste que escribe ha llegado -más por tozudez que por talento natural- a conocer un poco los entresijos de la programación asíncrona. Una disciplina en la que tan importante es saber lo que hay que hacer, como lo que no hay que hacer. Así pues, cuando llegó a mis manos la primera preview de Visual Studio 2010, una de las primeras cosas que hice fue mirar que demonios era eso de la Task Parallel Library. Primero porque ya hacía un tiempo que había escuchado acerca de la muerte anunciada de la ley de Moore. Y segundo porque cualquier cosa que hiciese más llevadero el trabajo de realizar y depurar aplicaciones multihilo, bienvenido iba a ser. Task Parallel Library Desde entonces hasta ahora he tenido la suerte de poder dedicar un poco de tiempo a esta maravilla, de modo que me he propuesto crear una serie bastante larga de posts y vídeos sobre el tema. Más que nada porque a mi juicio hay poca documentación (al menos en Español) y algo así no puede quedar relegado al olvido. Y es que en multitud de ocasiones que cuando le explico a alguien el porqué es importante y en que consiste la TPL, acostumbra a decir: “Que guapo! Pues no tenía ni idea…”. Y eso, me mata. ","date":"2011-01-11","objectID":"/es/parallelseries01-el-alfa/:1:0","tags":["csharp","net framework","Task","Parallel","Async"],"title":"Parallel Series 01 - El Alfa","uri":"/es/parallelseries01-el-alfa/"},{"categories":["Development","Parallel Series"],"content":"Nacen las Parallel Series En los próximos posts voy a ir definiendo el índice de contenidos de la serie, aunque a medida que crezca la serie es más que probable que vaya siendo modificado y ampliado. También veremos algo de historia para ponernos en contexto, aclararemos conceptos base que van a ser necesarios más adelante, e iremos desgranando los apartados principales de la Task Parallel Library. La idea es empezar desde abajo e ir subiendo de nivel, hasta llegar a los aspectos más complejos, como los problemas de concurrencia o la depuración de este tipo de aplicaciones. En cada apartado prometo poner al menos un ejemplo y si puedo más, porque los humanos -y los developers heredamos de esta clase base- aprendemos mucho mejor los conceptos teóricos si van acompañados de la práctica. Nos vemos muy pronto ;-) A continución… En el próximo post repasaremos la historia de la programación paralela y veremos cómo hemos llegado aquí. Ir al índice de la serie ","date":"2011-01-11","objectID":"/es/parallelseries01-el-alfa/:2:0","tags":["csharp","net framework","Task","Parallel","Async"],"title":"Parallel Series 01 - El Alfa","uri":"/es/parallelseries01-el-alfa/"},{"categories":["Development","Parallel Series"],"content":"Índice de contenidos 01 - El Alfa (Prólogo) 02 - Un poco de historia 03 - Aclarando conceptos base 04 - PLINQ: Parallel LINQ 05 - Parallel static class 06 - Tasks, la 8ª maravilla 07 - Problemas de concurrencia 08 - Materiales y presentaciones de mis eventos ","date":"2011-01-10","objectID":"/es/parallelseries00-index/:1:0","tags":["csharp","net framework","Task","Parallel","Async"],"title":"Parallel Series - Indice","uri":"/es/parallelseries00-index/"},{"categories":["Development","Parallel Series"],"content":"Relacionados Programación funcional para el resto de nosotros Luces, cámara… Action! ","date":"2011-01-10","objectID":"/es/parallelseries00-index/:2:0","tags":["csharp","net framework","Task","Parallel","Async"],"title":"Parallel Series - Indice","uri":"/es/parallelseries00-index/"}]